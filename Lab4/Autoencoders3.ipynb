{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense NET1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6026 images belonging to 101 classes.\n",
      "Found 2651 images belonging to 101 classes.\n",
      "WARNING:tensorflow:From C:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1205: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From C:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1290: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Epoch 1/35\n",
      "377/376 [==============================] - 119s - loss: 0.1024   \n",
      "Epoch 2/35\n",
      "377/376 [==============================] - 14s - loss: 0.1018    \n",
      "Epoch 3/35\n",
      "377/376 [==============================] - 14s - loss: 0.1008    \n",
      "Epoch 4/35\n",
      "377/376 [==============================] - 14s - loss: 0.0998    \n",
      "Epoch 5/35\n",
      "377/376 [==============================] - 14s - loss: 0.0985    \n",
      "Epoch 6/35\n",
      "377/376 [==============================] - 14s - loss: 0.0972    \n",
      "Epoch 7/35\n",
      "377/376 [==============================] - 14s - loss: 0.0960    \n",
      "Epoch 8/35\n",
      "377/376 [==============================] - 14s - loss: 0.0945    \n",
      "Epoch 9/35\n",
      "377/376 [==============================] - 14s - loss: 0.0934    \n",
      "Epoch 10/35\n",
      "377/376 [==============================] - 14s - loss: 0.0920    \n",
      "Epoch 11/35\n",
      "377/376 [==============================] - 14s - loss: 0.0912    \n",
      "Epoch 12/35\n",
      "377/376 [==============================] - 14s - loss: 0.0900    \n",
      "Epoch 13/35\n",
      "377/376 [==============================] - 14s - loss: 0.0890    \n",
      "Epoch 14/35\n",
      "377/376 [==============================] - 14s - loss: 0.0882    \n",
      "Epoch 15/35\n",
      "377/376 [==============================] - 14s - loss: 0.0873    \n",
      "Epoch 16/35\n",
      "377/376 [==============================] - 14s - loss: 0.0868    \n",
      "Epoch 17/35\n",
      "377/376 [==============================] - 14s - loss: 0.0859    \n",
      "Epoch 18/35\n",
      "377/376 [==============================] - 15s - loss: 0.0857    \n",
      "Epoch 19/35\n",
      "377/376 [==============================] - 14s - loss: 0.0848    \n",
      "Epoch 20/35\n",
      "377/376 [==============================] - 14s - loss: 0.0842    \n",
      "Epoch 21/35\n",
      "377/376 [==============================] - 14s - loss: 0.0838    \n",
      "Epoch 22/35\n",
      "377/376 [==============================] - 14s - loss: 0.0835    \n",
      "Epoch 23/35\n",
      "377/376 [==============================] - 14s - loss: 0.0830    \n",
      "Epoch 24/35\n",
      "377/376 [==============================] - 14s - loss: 0.0825    \n",
      "Epoch 25/35\n",
      "377/376 [==============================] - 14s - loss: 0.0824    \n",
      "Epoch 26/35\n",
      "377/376 [==============================] - 14s - loss: 0.0819    \n",
      "Epoch 27/35\n",
      "377/376 [==============================] - 14s - loss: 0.0816    \n",
      "Epoch 28/35\n",
      "377/376 [==============================] - 14s - loss: 0.0810    \n",
      "Epoch 29/35\n",
      "377/376 [==============================] - 14s - loss: 0.0812    \n",
      "Epoch 30/35\n",
      "377/376 [==============================] - 14s - loss: 0.0807    \n",
      "Epoch 31/35\n",
      "377/376 [==============================] - 14s - loss: 0.0803    \n",
      "Epoch 32/35\n",
      "377/376 [==============================] - 14s - loss: 0.0800    \n",
      "Epoch 33/35\n",
      "377/376 [==============================] - 14s - loss: 0.0799    \n",
      "Epoch 34/35\n",
      "377/376 [==============================] - 14s - loss: 0.0797    \n",
      "Epoch 35/35\n",
      "377/376 [==============================] - 14s - loss: 0.0793    \n",
      "Epoch 1/35\n",
      "189/188 [==============================] - 7s - loss: 0.0792     \n",
      "Epoch 2/35\n",
      "189/188 [==============================] - 7s - loss: 0.0789     \n",
      "Epoch 3/35\n",
      "189/188 [==============================] - 7s - loss: 0.0792     \n",
      "Epoch 4/35\n",
      "189/188 [==============================] - 7s - loss: 0.0784     \n",
      "Epoch 5/35\n",
      "189/188 [==============================] - 7s - loss: 0.0789     \n",
      "Epoch 6/35\n",
      "189/188 [==============================] - 7s - loss: 0.0783     \n",
      "Epoch 7/35\n",
      "189/188 [==============================] - 7s - loss: 0.0787     \n",
      "Epoch 8/35\n",
      "189/188 [==============================] - 7s - loss: 0.0784     \n",
      "Epoch 9/35\n",
      "189/188 [==============================] - 7s - loss: 0.0780     \n",
      "Epoch 10/35\n",
      "189/188 [==============================] - 7s - loss: 0.0783     \n",
      "Epoch 11/35\n",
      "189/188 [==============================] - 7s - loss: 0.0781     \n",
      "Epoch 12/35\n",
      "189/188 [==============================] - 7s - loss: 0.0778     \n",
      "Epoch 13/35\n",
      "189/188 [==============================] - 7s - loss: 0.0782     \n",
      "Epoch 14/35\n",
      "189/188 [==============================] - 7s - loss: 0.0773     \n",
      "Epoch 15/35\n",
      "189/188 [==============================] - 7s - loss: 0.0775     \n",
      "Epoch 16/35\n",
      "189/188 [==============================] - 7s - loss: 0.0775     \n",
      "Epoch 17/35\n",
      "189/188 [==============================] - 7s - loss: 0.0773     \n",
      "Epoch 18/35\n",
      "189/188 [==============================] - 7s - loss: 0.0774     \n",
      "Epoch 19/35\n",
      "189/188 [==============================] - 7s - loss: 0.0771     \n",
      "Epoch 20/35\n",
      "189/188 [==============================] - 7s - loss: 0.0773     \n",
      "Epoch 21/35\n",
      "189/188 [==============================] - 7s - loss: 0.0767     \n",
      "Epoch 22/35\n",
      "189/188 [==============================] - 7s - loss: 0.0768     \n",
      "Epoch 23/35\n",
      "189/188 [==============================] - 7s - loss: 0.0771     \n",
      "Epoch 24/35\n",
      "189/188 [==============================] - 7s - loss: 0.0769     \n",
      "Epoch 25/35\n",
      "189/188 [==============================] - 7s - loss: 0.0765     \n",
      "Epoch 26/35\n",
      "189/188 [==============================] - 7s - loss: 0.0766     \n",
      "Epoch 27/35\n",
      "189/188 [==============================] - 7s - loss: 0.0765     \n",
      "Epoch 28/35\n",
      "189/188 [==============================] - 7s - loss: 0.0764     \n",
      "Epoch 29/35\n",
      "189/188 [==============================] - 7s - loss: 0.0762     \n",
      "Epoch 30/35\n",
      "189/188 [==============================] - 7s - loss: 0.0761     \n",
      "Epoch 31/35\n",
      "189/188 [==============================] - 7s - loss: 0.0760     \n",
      "Epoch 32/35\n",
      "189/188 [==============================] - 7s - loss: 0.0759     \n",
      "Epoch 33/35\n",
      "189/188 [==============================] - 7s - loss: 0.0762     \n",
      "Epoch 34/35\n",
      "189/188 [==============================] - 7s - loss: 0.0757     \n",
      "Epoch 35/35\n",
      "189/188 [==============================] - 7s - loss: 0.0755     \n",
      "Epoch 1/35\n",
      "95/94 [==============================] - 4s - loss: 0.0760     \n",
      "Epoch 2/35\n",
      "95/94 [==============================] - 3s - loss: 0.0757     \n",
      "Epoch 3/35\n",
      "95/94 [==============================] - 3s - loss: 0.0751     \n",
      "Epoch 4/35\n",
      "95/94 [==============================] - 3s - loss: 0.0756     \n",
      "Epoch 5/35\n",
      "95/94 [==============================] - 3s - loss: 0.0757     \n",
      "Epoch 6/35\n",
      "95/94 [==============================] - 3s - loss: 0.0754     \n",
      "Epoch 7/35\n",
      "95/94 [==============================] - 3s - loss: 0.0759     \n",
      "Epoch 8/35\n",
      "95/94 [==============================] - 3s - loss: 0.0746     \n",
      "Epoch 9/35\n",
      "95/94 [==============================] - 3s - loss: 0.0757     \n",
      "Epoch 10/35\n",
      "95/94 [==============================] - 3s - loss: 0.0753     \n",
      "Epoch 11/35\n",
      "95/94 [==============================] - 3s - loss: 0.0758     \n",
      "Epoch 12/35\n",
      "95/94 [==============================] - 3s - loss: 0.0748     \n",
      "Epoch 13/35\n",
      "95/94 [==============================] - 3s - loss: 0.0751     \n",
      "Epoch 14/35\n",
      "95/94 [==============================] - 3s - loss: 0.0753     \n",
      "Epoch 15/35\n",
      "95/94 [==============================] - 3s - loss: 0.0748     \n",
      "Epoch 16/35\n",
      "95/94 [==============================] - 3s - loss: 0.0753     \n",
      "Epoch 17/35\n",
      "95/94 [==============================] - 3s - loss: 0.0740     \n",
      "Epoch 18/35\n",
      "95/94 [==============================] - 3s - loss: 0.0755     \n",
      "Epoch 19/35\n",
      "95/94 [==============================] - 3s - loss: 0.0746     \n",
      "Epoch 20/35\n",
      "95/94 [==============================] - 3s - loss: 0.0754     \n",
      "Epoch 21/35\n",
      "95/94 [==============================] - 3s - loss: 0.0748     \n",
      "Epoch 22/35\n",
      "95/94 [==============================] - 3s - loss: 0.0748     \n",
      "Epoch 23/35\n",
      "95/94 [==============================] - 3s - loss: 0.0748     \n",
      "Epoch 24/35\n",
      "95/94 [==============================] - 3s - loss: 0.0746     \n",
      "Epoch 25/35\n",
      "95/94 [==============================] - 3s - loss: 0.0752     \n",
      "Epoch 26/35\n",
      "95/94 [==============================] - 3s - loss: 0.0741     \n",
      "Epoch 27/35\n",
      "95/94 [==============================] - 3s - loss: 0.0749     \n",
      "Epoch 28/35\n",
      "95/94 [==============================] - 3s - loss: 0.0742     \n",
      "Epoch 29/35\n",
      "95/94 [==============================] - 3s - loss: 0.0744     \n",
      "Epoch 30/35\n",
      "95/94 [==============================] - 3s - loss: 0.0746     \n",
      "Epoch 31/35\n",
      "95/94 [==============================] - 3s - loss: 0.0740     \n",
      "Epoch 32/35\n",
      "95/94 [==============================] - 3s - loss: 0.0750     \n",
      "Epoch 33/35\n",
      "95/94 [==============================] - 3s - loss: 0.0741     \n",
      "Epoch 34/35\n",
      "95/94 [==============================] - 3s - loss: 0.0746     \n",
      "Epoch 35/35\n",
      "95/94 [==============================] - 3s - loss: 0.0749     \n",
      "Epoch 1/35\n",
      "189/188 [==============================] - 4s - loss: 0.0735     \n",
      "Epoch 2/35\n",
      "189/188 [==============================] - 4s - loss: 0.0743     \n",
      "Epoch 3/35\n",
      "189/188 [==============================] - 4s - loss: 0.0742     \n",
      "Epoch 4/35\n",
      "189/188 [==============================] - 4s - loss: 0.0739     \n",
      "Epoch 5/35\n",
      "189/188 [==============================] - 4s - loss: 0.0738     \n",
      "Epoch 6/35\n",
      "189/188 [==============================] - 4s - loss: 0.0742     \n",
      "Epoch 7/35\n",
      "189/188 [==============================] - 4s - loss: 0.0737     \n",
      "Epoch 8/35\n",
      "189/188 [==============================] - 4s - loss: 0.0730     \n",
      "Epoch 9/35\n",
      "189/188 [==============================] - 4s - loss: 0.0742     \n",
      "Epoch 10/35\n",
      "189/188 [==============================] - 4s - loss: 0.0732     \n",
      "Epoch 11/35\n",
      "189/188 [==============================] - 4s - loss: 0.0737     \n",
      "Epoch 12/35\n",
      "189/188 [==============================] - 4s - loss: 0.0734     \n",
      "Epoch 13/35\n",
      "189/188 [==============================] - 4s - loss: 0.0731     \n",
      "Epoch 14/35\n",
      "189/188 [==============================] - 4s - loss: 0.0730     \n",
      "Epoch 15/35\n",
      "189/188 [==============================] - 4s - loss: 0.0733     \n",
      "Epoch 16/35\n",
      "189/188 [==============================] - 4s - loss: 0.0733     \n",
      "Epoch 17/35\n",
      "189/188 [==============================] - 4s - loss: 0.0727     \n",
      "Epoch 18/35\n",
      "189/188 [==============================] - 4s - loss: 0.0730     \n",
      "Epoch 19/35\n",
      "189/188 [==============================] - 4s - loss: 0.0730     \n",
      "Epoch 20/35\n",
      "189/188 [==============================] - 4s - loss: 0.0727     \n",
      "Epoch 21/35\n",
      "189/188 [==============================] - 4s - loss: 0.0729     \n",
      "Epoch 22/35\n",
      "189/188 [==============================] - 4s - loss: 0.0728     \n",
      "Epoch 23/35\n",
      "189/188 [==============================] - 4s - loss: 0.0723     \n",
      "Epoch 24/35\n",
      "189/188 [==============================] - 4s - loss: 0.0728     \n",
      "Epoch 25/35\n",
      "189/188 [==============================] - 4s - loss: 0.0722     \n",
      "Epoch 26/35\n",
      "189/188 [==============================] - 4s - loss: 0.0727     \n",
      "Epoch 27/35\n",
      "189/188 [==============================] - 4s - loss: 0.0716     \n",
      "Epoch 28/35\n",
      "189/188 [==============================] - 4s - loss: 0.0728     \n",
      "Epoch 29/35\n",
      "189/188 [==============================] - 4s - loss: 0.0715     \n",
      "Epoch 30/35\n",
      "189/188 [==============================] - 4s - loss: 0.0728     \n",
      "Epoch 31/35\n",
      "189/188 [==============================] - 4s - loss: 0.0717     \n",
      "Epoch 32/35\n",
      "189/188 [==============================] - 4s - loss: 0.0719     \n",
      "Epoch 33/35\n",
      "189/188 [==============================] - 4s - loss: 0.0725     \n",
      "Epoch 34/35\n",
      "189/188 [==============================] - 4s - loss: 0.0713     \n",
      "Epoch 35/35\n",
      "189/188 [==============================] - 4s - loss: 0.0720     \n",
      "Epoch 1/35\n",
      "189/188 [==============================] - 4s - loss: 0.0713     \n",
      "Epoch 2/35\n",
      "189/188 [==============================] - 4s - loss: 0.0719     \n",
      "Epoch 3/35\n",
      "189/188 [==============================] - 4s - loss: 0.0720     \n",
      "Epoch 4/35\n",
      "189/188 [==============================] - 4s - loss: 0.0713     \n",
      "Epoch 5/35\n",
      "189/188 [==============================] - 4s - loss: 0.0716     \n",
      "Epoch 6/35\n",
      "189/188 [==============================] - 4s - loss: 0.0719     \n",
      "Epoch 7/35\n",
      "189/188 [==============================] - 4s - loss: 0.0718     \n",
      "Epoch 8/35\n",
      "189/188 [==============================] - 4s - loss: 0.0721     \n",
      "Epoch 9/35\n",
      "189/188 [==============================] - 4s - loss: 0.0711     \n",
      "Epoch 10/35\n",
      "189/188 [==============================] - 4s - loss: 0.0714     \n",
      "Epoch 11/35\n",
      "189/188 [==============================] - 4s - loss: 0.0717     \n",
      "Epoch 12/35\n",
      "189/188 [==============================] - 4s - loss: 0.0717     \n",
      "Epoch 13/35\n",
      "189/188 [==============================] - 4s - loss: 0.0719     \n",
      "Epoch 14/35\n",
      "189/188 [==============================] - 4s - loss: 0.0716     \n",
      "Epoch 15/35\n",
      "189/188 [==============================] - 4s - loss: 0.0715     \n",
      "Epoch 16/35\n",
      "189/188 [==============================] - 4s - loss: 0.0717     \n",
      "Epoch 17/35\n",
      "189/188 [==============================] - 4s - loss: 0.0713     \n",
      "Epoch 18/35\n",
      "189/188 [==============================] - 4s - loss: 0.0717     \n",
      "Epoch 19/35\n",
      "189/188 [==============================] - 4s - loss: 0.0718     \n",
      "Epoch 20/35\n",
      "189/188 [==============================] - 4s - loss: 0.0713     \n",
      "Epoch 21/35\n",
      "189/188 [==============================] - 4s - loss: 0.0719     \n",
      "Epoch 22/35\n",
      "189/188 [==============================] - 4s - loss: 0.0715     \n",
      "Epoch 23/35\n",
      "189/188 [==============================] - 4s - loss: 0.0718     \n",
      "Epoch 24/35\n",
      "189/188 [==============================] - 4s - loss: 0.0715     \n",
      "Epoch 25/35\n",
      "189/188 [==============================] - 4s - loss: 0.0710     \n",
      "Epoch 26/35\n",
      "189/188 [==============================] - 4s - loss: 0.0720     \n",
      "Epoch 27/35\n",
      "189/188 [==============================] - 4s - loss: 0.0714     \n",
      "Epoch 28/35\n",
      "189/188 [==============================] - 4s - loss: 0.0712     \n",
      "Epoch 29/35\n",
      "189/188 [==============================] - 4s - loss: 0.0723     \n",
      "Epoch 30/35\n",
      "189/188 [==============================] - 4s - loss: 0.0714     \n",
      "Epoch 31/35\n",
      "189/188 [==============================] - 4s - loss: 0.0716     \n",
      "Epoch 32/35\n",
      "189/188 [==============================] - 4s - loss: 0.0713     \n",
      "Epoch 33/35\n",
      "189/188 [==============================] - 4s - loss: 0.0715     \n",
      "Epoch 34/35\n",
      "189/188 [==============================] - 4s - loss: 0.0714     \n",
      "Epoch 35/35\n",
      "189/188 [==============================] - 4s - loss: 0.0714     \n",
      "Epoch 1/35\n",
      "189/188 [==============================] - 4s - loss: 0.0718     \n",
      "Epoch 2/35\n",
      "189/188 [==============================] - 4s - loss: 0.0713     \n",
      "Epoch 3/35\n",
      "189/188 [==============================] - 4s - loss: 0.0716     \n",
      "Epoch 4/35\n",
      "189/188 [==============================] - 4s - loss: 0.0713     \n",
      "Epoch 5/35\n",
      "189/188 [==============================] - 4s - loss: 0.0716     \n",
      "Epoch 6/35\n",
      "189/188 [==============================] - 4s - loss: 0.0711     \n",
      "Epoch 7/35\n",
      "189/188 [==============================] - 4s - loss: 0.0720     \n",
      "Epoch 8/35\n",
      "189/188 [==============================] - 4s - loss: 0.0717     \n",
      "Epoch 9/35\n",
      "189/188 [==============================] - 4s - loss: 0.0714     \n",
      "Epoch 10/35\n",
      "189/188 [==============================] - 4s - loss: 0.0716     \n",
      "Epoch 11/35\n",
      "189/188 [==============================] - 4s - loss: 0.0713     \n",
      "Epoch 12/35\n",
      "189/188 [==============================] - 4s - loss: 0.0714     \n",
      "Epoch 13/35\n",
      "189/188 [==============================] - 4s - loss: 0.0716     \n",
      "Epoch 14/35\n",
      "189/188 [==============================] - 4s - loss: 0.0717     \n",
      "Epoch 15/35\n",
      "189/188 [==============================] - 4s - loss: 0.0709     \n",
      "Epoch 16/35\n",
      "189/188 [==============================] - 4s - loss: 0.0718     \n",
      "Epoch 17/35\n",
      "189/188 [==============================] - 4s - loss: 0.0718     \n",
      "Epoch 18/35\n",
      "189/188 [==============================] - 4s - loss: 0.0709     \n",
      "Epoch 19/35\n",
      "189/188 [==============================] - 4s - loss: 0.0714     \n",
      "Epoch 20/35\n",
      "189/188 [==============================] - 4s - loss: 0.0720     \n",
      "Epoch 21/35\n",
      "189/188 [==============================] - 4s - loss: 0.0716     \n",
      "Epoch 22/35\n",
      "189/188 [==============================] - 4s - loss: 0.0712     \n",
      "Epoch 23/35\n",
      "189/188 [==============================] - 4s - loss: 0.0718     \n",
      "Epoch 24/35\n",
      "189/188 [==============================] - 4s - loss: 0.0714     \n",
      "Epoch 25/35\n",
      "189/188 [==============================] - 4s - loss: 0.0711     \n",
      "Epoch 26/35\n",
      "189/188 [==============================] - 4s - loss: 0.0719     \n",
      "Epoch 27/35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189/188 [==============================] - 4s - loss: 0.0714     \n",
      "Epoch 28/35\n",
      "189/188 [==============================] - 4s - loss: 0.0716     \n",
      "Epoch 29/35\n",
      "189/188 [==============================] - 4s - loss: 0.0711     \n",
      "Epoch 30/35\n",
      "189/188 [==============================] - 4s - loss: 0.0719     \n",
      "Epoch 31/35\n",
      "189/188 [==============================] - 4s - loss: 0.0717     \n",
      "Epoch 32/35\n",
      "189/188 [==============================] - 4s - loss: 0.0716     \n",
      "Epoch 33/35\n",
      "189/188 [==============================] - 4s - loss: 0.0711     \n",
      "Epoch 34/35\n",
      "189/188 [==============================] - 4s - loss: 0.0715     \n",
      "Epoch 35/35\n",
      "189/188 [==============================] - 4s - loss: 0.0717     \n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Input, Dense, Flatten, Reshape, Dropout, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import SGD\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "def tuple_generator(generator):\n",
    "    for batch in generator:\n",
    "        yield (batch, batch)\n",
    "\n",
    "np.random.seed(777)\n",
    "#if (len(sys.argv)<3):\n",
    "#    print(\"Input arguments:\")\n",
    "#    print(\"1. Train images path\")\n",
    "#    print(\"2. Test images path\")\n",
    "#    exit()\n",
    "PreTrainImagesPath=\"D:/SomeData/Caltech101/TrainImages\"\n",
    "PreTestImagesPath=\"D:/SomeData/Caltech101/TestImages\"\n",
    "TrainImagesPath=\"D:/SomeData/Caltech101/TrainImages\"\n",
    "TestImagesPath=\"D:/SomeData/Caltech101/TestImages\"\n",
    "img_width, img_height = 128, 128\n",
    "epochs = 35\n",
    "batch_size_1 = 16\n",
    "batch_size = 32\n",
    "batch_size_3 = 64\n",
    "dropout_rate = 0.3\n",
    "latent_dim = 101\n",
    "\n",
    "datagen=ImageDataGenerator(rescale=1./255)\n",
    "pretrain_generator = datagen.flow_from_directory(\n",
    "        PreTrainImagesPath,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None)\n",
    "train_samples = pretrain_generator.n\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "        PreTestImagesPath,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None)\n",
    "nb_validation_samples = validation_generator.n\n",
    "\n",
    "sgd_4 = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "sgd_5 = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "sgd_6 = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "input_img = Input(shape=(img_width, img_height, 3))\n",
    "x = Flatten(input_shape=(img_width, img_height, 3))(input_img)\n",
    "encoded = (Dense(units=300, activation='sigmoid', name='dens_sigmoid_1'))(x)\n",
    "x = Dense(img_width*img_height*3, activation='sigmoid')(encoded)\n",
    "decoded = Reshape((img_width, img_height, 3), input_shape=(img_width*img_height*3,))(x)\n",
    "autoencoder_1 = Model(input_img, decoded)\n",
    "autoencoder_1.compile(optimizer='adadelta', loss='mse')\n",
    "#print (autoencoder.summary())\n",
    "autoencoder_1.fit_generator(\n",
    "        tuple_generator(pretrain_generator),\n",
    "        steps_per_epoch=train_samples/batch_size_1,\n",
    "        epochs=epochs\n",
    "        )\n",
    "#autoencoder_1.save('DenseAutoEncoderNET1')\n",
    "autoencoder_1.save_weights('DenseAutoEncoderNET1_weights_1')\n",
    "\n",
    "#plot_model(autoencoder, to_file='DenseAutoEncoder.png', show_shapes=True, show_layer_names=False, rankdir='LR')\n",
    "\n",
    "autoencoder_2 = Model(input_img, decoded)\n",
    "autoencoder_2.compile(optimizer='adadelta', loss='mse')\n",
    "autoencoder_2.fit_generator(\n",
    "        tuple_generator(pretrain_generator),\n",
    "        steps_per_epoch=train_samples/batch_size,\n",
    "        epochs=epochs\n",
    "        )\n",
    "autoencoder_2.save_weights('DenseAutoEncoderNET1_weights_2')\n",
    "\n",
    "autoencoder_3 = Model(input_img, decoded)\n",
    "autoencoder_3.compile(optimizer='adadelta', loss='mse')\n",
    "autoencoder_3.fit_generator(\n",
    "        tuple_generator(pretrain_generator),\n",
    "        steps_per_epoch=train_samples/batch_size_3,\n",
    "        epochs=epochs\n",
    "        )\n",
    "autoencoder_3.save_weights('DenseAutoEncoderNET1_weights_3')\n",
    "\n",
    "autoencoder_4 = Model(input_img, decoded)\n",
    "autoencoder_4.compile(optimizer=sgd_4, loss='mse')\n",
    "autoencoder_4.fit_generator(\n",
    "        tuple_generator(pretrain_generator),\n",
    "        steps_per_epoch=train_samples/batch_size,\n",
    "        epochs=epochs\n",
    "        )\n",
    "autoencoder_4.save_weights('DenseAutoEncoderNET1_weights_4')\n",
    "\n",
    "autoencoder_5 = Model(input_img, decoded)\n",
    "autoencoder_5.compile(optimizer=sgd_5, loss='mse')\n",
    "autoencoder_5.fit_generator(\n",
    "        tuple_generator(pretrain_generator),\n",
    "        steps_per_epoch=train_samples/batch_size,\n",
    "        epochs=epochs\n",
    "        )\n",
    "autoencoder_5.save_weights('DenseAutoEncoderNET1_weights_5')\n",
    "\n",
    "autoencoder_6 = Model(input_img, decoded)\n",
    "autoencoder_6.compile(optimizer=sgd_6, loss='mse')\n",
    "autoencoder_6.fit_generator(\n",
    "        tuple_generator(pretrain_generator),\n",
    "        steps_per_epoch=train_samples/batch_size,\n",
    "        epochs=epochs\n",
    "        )\n",
    "autoencoder_6.save_weights('DenseAutoEncoderNET1_weights_6')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "K.clear_session()\n",
    "sess = tf.Session()\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6026 images belonging to 101 classes.\n",
      "Found 2651 images belonging to 101 classes.\n",
      "WARNING:tensorflow:From C:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2755: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Epoch 1/35\n",
      "189/188 [==============================] - 12s - loss: 3.8608 - acc: 0.2099    \n",
      "Epoch 2/35\n",
      "189/188 [==============================] - 12s - loss: 3.3700 - acc: 0.2828    \n",
      "Epoch 3/35\n",
      "189/188 [==============================] - 12s - loss: 3.1446 - acc: 0.3136    \n",
      "Epoch 4/35\n",
      "189/188 [==============================] - 12s - loss: 2.9785 - acc: 0.3388    \n",
      "Epoch 5/35\n",
      "189/188 [==============================] - 12s - loss: 2.8342 - acc: 0.3723    \n",
      "Epoch 6/35\n",
      "189/188 [==============================] - 12s - loss: 2.7122 - acc: 0.4003    \n",
      "Epoch 7/35\n",
      "189/188 [==============================] - 12s - loss: 2.5922 - acc: 0.4356    \n",
      "Epoch 8/35\n",
      "189/188 [==============================] - 12s - loss: 2.4885 - acc: 0.4622    \n",
      "Epoch 9/35\n",
      "189/188 [==============================] - 12s - loss: 2.3873 - acc: 0.4927    \n",
      "Epoch 10/35\n",
      "189/188 [==============================] - 12s - loss: 2.2908 - acc: 0.5227    \n",
      "Epoch 11/35\n",
      "189/188 [==============================] - 12s - loss: 2.2000 - acc: 0.5503    \n",
      "Epoch 12/35\n",
      "189/188 [==============================] - 12s - loss: 2.1126 - acc: 0.5767    \n",
      "Epoch 13/35\n",
      "189/188 [==============================] - 12s - loss: 2.0357 - acc: 0.6060    \n",
      "Epoch 14/35\n",
      "189/188 [==============================] - 12s - loss: 1.9577 - acc: 0.6313    \n",
      "Epoch 15/35\n",
      "189/188 [==============================] - 12s - loss: 1.8864 - acc: 0.6464    \n",
      "Epoch 16/35\n",
      "189/188 [==============================] - 12s - loss: 1.8135 - acc: 0.6665    \n",
      "Epoch 17/35\n",
      "189/188 [==============================] - 12s - loss: 1.7431 - acc: 0.6911    \n",
      "Epoch 18/35\n",
      "189/188 [==============================] - 12s - loss: 1.6867 - acc: 0.7083    \n",
      "Epoch 19/35\n",
      "189/188 [==============================] - 12s - loss: 1.6222 - acc: 0.7294    -\n",
      "Epoch 20/35\n",
      "189/188 [==============================] - 12s - loss: 1.5639 - acc: 0.7439    \n",
      "Epoch 21/35\n",
      "189/188 [==============================] - 12s - loss: 1.5057 - acc: 0.7678    \n",
      "Epoch 22/35\n",
      "189/188 [==============================] - 12s - loss: 1.4533 - acc: 0.7787    \n",
      "Epoch 23/35\n",
      "189/188 [==============================] - 12s - loss: 1.4045 - acc: 0.7920    \n",
      "Epoch 24/35\n",
      "189/188 [==============================] - 12s - loss: 1.3501 - acc: 0.8073    \n",
      "Epoch 25/35\n",
      "189/188 [==============================] - 12s - loss: 1.3055 - acc: 0.8169    \n",
      "Epoch 26/35\n",
      "189/188 [==============================] - 12s - loss: 1.2593 - acc: 0.8217    \n",
      "Epoch 27/35\n",
      "189/188 [==============================] - 12s - loss: 1.2140 - acc: 0.8334    \n",
      "Epoch 28/35\n",
      "189/188 [==============================] - 12s - loss: 1.1717 - acc: 0.8449    \n",
      "Epoch 29/35\n",
      "189/188 [==============================] - 12s - loss: 1.1369 - acc: 0.8510    \n",
      "Epoch 30/35\n",
      "189/188 [==============================] - 12s - loss: 1.0959 - acc: 0.8594    \n",
      "Epoch 31/35\n",
      "189/188 [==============================] - 12s - loss: 1.0602 - acc: 0.8664    \n",
      "Epoch 32/35\n",
      "189/188 [==============================] - 12s - loss: 1.0307 - acc: 0.8695    \n",
      "Epoch 33/35\n",
      "189/188 [==============================] - 13s - loss: 0.9946 - acc: 0.8769    \n",
      "Epoch 34/35\n",
      "189/188 [==============================] - 13s - loss: 0.9617 - acc: 0.8821    \n",
      "Epoch 35/35\n",
      "189/188 [==============================] - 12s - loss: 0.9335 - acc: 0.8869    \n",
      "Accuracy = 0.313089400226\n",
      "Time = 437.640753030777\n"
     ]
    }
   ],
   "source": [
    "datagen=ImageDataGenerator(samplewise_center=True,\n",
    "    samplewise_std_normalization=True)\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "        TrainImagesPath,\n",
    "        target_size=(128, 128),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical', seed=777)\n",
    "train_count = train_generator.n\n",
    "test_generator = datagen.flow_from_directory(\n",
    "        TestImagesPath,\n",
    "        target_size=(128, 128),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',shuffle=False)\n",
    "test_count = test_generator.n\n",
    "#%%\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(128,128,3),name='flatten'))\n",
    "model.add(Dense(units=300, activation='sigmoid', name='dens_sigmoid_1'))\n",
    "model.add(Dense(units=101, activation='softmax'))\n",
    "#model.load_weights('DenseAutoEncoderNET1_weights', by_name=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "#%%\n",
    "early_stopping=EarlyStopping(monitor='acc', patience=3, verbose=0, mode='auto')\n",
    "t0=time.time()\n",
    "model.fit_generator(train_generator,\n",
    "        steps_per_epoch=train_count/batch_size,\n",
    "        epochs=epochs,\n",
    "        callbacks=[early_stopping])\n",
    "t1=time.time()\n",
    "loss_and_metrics = model.evaluate_generator(test_generator, steps=test_count/batch_size)\n",
    "print('Accuracy =',loss_and_metrics[1])\n",
    "print('Time =',(t1-t0))\n",
    "#%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "K.clear_session()\n",
    "sess = tf.Session()\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.preprocessing.image.DirectoryIterator at 0x136c4b6d5c0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrain_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = Sequential()\n",
    "model_1.add(Flatten(input_shape=(128,128,3),name='flatten'))\n",
    "model_1.add(Dense(units=300, activation='sigmoid', name='dens_sigmoid_1'))\n",
    "model_1.add(Dense(units=101, activation='softmax'))\n",
    "model_1.load_weights('DenseAutoEncoderNET1_weights_1', by_name=True)\n",
    "model_1.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "189/188 [==============================] - 13s - loss: 4.1694 - acc: 0.1627    \n",
      "Epoch 2/35\n",
      "189/188 [==============================] - 12s - loss: 3.7617 - acc: 0.2263    \n",
      "Epoch 3/35\n",
      "189/188 [==============================] - 12s - loss: 3.5658 - acc: 0.2582    \n",
      "Epoch 4/35\n",
      "189/188 [==============================] - 12s - loss: 3.4252 - acc: 0.2780    \n",
      "Epoch 5/35\n",
      "189/188 [==============================] - 12s - loss: 3.3063 - acc: 0.2973    - ETA: 0s - loss: 3.3150\n",
      "Epoch 6/35\n",
      "189/188 [==============================] - 12s - loss: 3.2015 - acc: 0.3112    \n",
      "Epoch 7/35\n",
      "189/188 [==============================] - 12s - loss: 3.1069 - acc: 0.3237    \n",
      "Epoch 8/35\n",
      "189/188 [==============================] - 12s - loss: 3.0205 - acc: 0.3394    \n",
      "Epoch 9/35\n",
      "189/188 [==============================] - 12s - loss: 2.9399 - acc: 0.3542    \n",
      "Epoch 10/35\n",
      "189/188 [==============================] - 12s - loss: 2.8654 - acc: 0.3721    \n",
      "Epoch 11/35\n",
      "189/188 [==============================] - 12s - loss: 2.7816 - acc: 0.3912    \n",
      "Epoch 12/35\n",
      "189/188 [==============================] - 12s - loss: 2.7195 - acc: 0.4006    \n",
      "Epoch 13/35\n",
      "189/188 [==============================] - 12s - loss: 2.6506 - acc: 0.4191    \n",
      "Epoch 14/35\n",
      "189/188 [==============================] - 12s - loss: 2.5807 - acc: 0.4364    \n",
      "Epoch 15/35\n",
      "189/188 [==============================] - 12s - loss: 2.5203 - acc: 0.4583    \n",
      "Epoch 16/35\n",
      "189/188 [==============================] - 11s - loss: 2.4621 - acc: 0.4790    \n",
      "Epoch 17/35\n",
      "189/188 [==============================] - 12s - loss: 2.4052 - acc: 0.4924    \n",
      "Epoch 18/35\n",
      "189/188 [==============================] - 12s - loss: 2.3470 - acc: 0.5133    \n",
      "Epoch 19/35\n",
      "189/188 [==============================] - 12s - loss: 2.2861 - acc: 0.5300    \n",
      "Epoch 20/35\n",
      "189/188 [==============================] - 12s - loss: 2.2339 - acc: 0.5482    \n",
      "Epoch 21/35\n",
      "189/188 [==============================] - 12s - loss: 2.1819 - acc: 0.5642    \n",
      "Epoch 22/35\n",
      "189/188 [==============================] - 12s - loss: 2.1274 - acc: 0.5820    \n",
      "Epoch 23/35\n",
      "189/188 [==============================] - 12s - loss: 2.0819 - acc: 0.5985    \n",
      "Epoch 24/35\n",
      "189/188 [==============================] - 12s - loss: 2.0279 - acc: 0.6158    \n",
      "Epoch 25/35\n",
      "189/188 [==============================] - 12s - loss: 1.9761 - acc: 0.6316    \n",
      "Epoch 26/35\n",
      "189/188 [==============================] - 12s - loss: 1.9315 - acc: 0.6483    \n",
      "Epoch 27/35\n",
      "189/188 [==============================] - 13s - loss: 1.8879 - acc: 0.6698    \n",
      "Epoch 28/35\n",
      "189/188 [==============================] - 12s - loss: 1.8451 - acc: 0.6843    \n",
      "Epoch 29/35\n",
      "189/188 [==============================] - 13s - loss: 1.7905 - acc: 0.7013    \n",
      "Epoch 30/35\n",
      "189/188 [==============================] - 12s - loss: 1.7523 - acc: 0.7126    \n",
      "Epoch 31/35\n",
      "189/188 [==============================] - 12s - loss: 1.7115 - acc: 0.7312    \n",
      "Epoch 32/35\n",
      "189/188 [==============================] - 12s - loss: 1.6652 - acc: 0.7397    \n",
      "Epoch 33/35\n",
      "189/188 [==============================] - 12s - loss: 1.6226 - acc: 0.7546    \n",
      "Epoch 34/35\n",
      "189/188 [==============================] - 12s - loss: 1.5828 - acc: 0.7645    \n",
      "Epoch 35/35\n",
      "189/188 [==============================] - 12s - loss: 1.5465 - acc: 0.7710    \n",
      "Accuracy = 0.311957751792\n",
      "Time = 438.6627595424652\n",
      "Epoch 1/35\n",
      "189/188 [==============================] - 12s - loss: 4.2204 - acc: 0.1484    \n",
      "Epoch 2/35\n",
      "189/188 [==============================] - 12s - loss: 3.8719 - acc: 0.2160    \n",
      "Epoch 3/35\n",
      "189/188 [==============================] - 12s - loss: 3.6923 - acc: 0.2410    \n",
      "Epoch 4/35\n",
      "189/188 [==============================] - 12s - loss: 3.5685 - acc: 0.2575    \n",
      "Epoch 5/35\n",
      "189/188 [==============================] - 12s - loss: 3.4484 - acc: 0.2782    \n",
      "Epoch 6/35\n",
      "189/188 [==============================] - 12s - loss: 3.3396 - acc: 0.2881    \n",
      "Epoch 7/35\n",
      "189/188 [==============================] - 12s - loss: 3.2414 - acc: 0.3023    \n",
      "Epoch 8/35\n",
      "189/188 [==============================] - 12s - loss: 3.1612 - acc: 0.3192    \n",
      "Epoch 9/35\n",
      "189/188 [==============================] - 12s - loss: 3.0702 - acc: 0.3332    \n",
      "Epoch 10/35\n",
      "189/188 [==============================] - 12s - loss: 3.0019 - acc: 0.3439    \n",
      "Epoch 11/35\n",
      "189/188 [==============================] - 12s - loss: 2.9250 - acc: 0.3606    \n",
      "Epoch 12/35\n",
      "189/188 [==============================] - 12s - loss: 2.8579 - acc: 0.3749    \n",
      "Epoch 13/35\n",
      "189/188 [==============================] - 12s - loss: 2.7930 - acc: 0.3930    \n",
      "Epoch 14/35\n",
      "189/188 [==============================] - 12s - loss: 2.7168 - acc: 0.4110    \n",
      "Epoch 15/35\n",
      "189/188 [==============================] - 12s - loss: 2.6660 - acc: 0.4238    \n",
      "Epoch 16/35\n",
      "189/188 [==============================] - 12s - loss: 2.6033 - acc: 0.4422    \n",
      "Epoch 17/35\n",
      "189/188 [==============================] - 12s - loss: 2.5554 - acc: 0.4547    \n",
      "Epoch 18/35\n",
      "189/188 [==============================] - 12s - loss: 2.4841 - acc: 0.4734    \n",
      "Epoch 19/35\n",
      "189/188 [==============================] - 12s - loss: 2.4291 - acc: 0.4924    \n",
      "Epoch 20/35\n",
      "189/188 [==============================] - 12s - loss: 2.3707 - acc: 0.5043    \n",
      "Epoch 21/35\n",
      "189/188 [==============================] - 12s - loss: 2.3315 - acc: 0.5213    \n",
      "Epoch 22/35\n",
      "189/188 [==============================] - 12s - loss: 2.2728 - acc: 0.5419    \n",
      "Epoch 23/35\n",
      "189/188 [==============================] - 12s - loss: 2.2196 - acc: 0.5572    \n",
      "Epoch 24/35\n",
      "189/188 [==============================] - 12s - loss: 2.1728 - acc: 0.5723    \n",
      "Epoch 25/35\n",
      "189/188 [==============================] - 12s - loss: 2.1277 - acc: 0.5929    \n",
      "Epoch 26/35\n",
      "189/188 [==============================] - 12s - loss: 2.0774 - acc: 0.6043    \n",
      "Epoch 27/35\n",
      "189/188 [==============================] - 13s - loss: 2.0360 - acc: 0.6156    \n",
      "Epoch 28/35\n",
      "189/188 [==============================] - 12s - loss: 1.9908 - acc: 0.6299    \n",
      "Epoch 29/35\n",
      "189/188 [==============================] - 12s - loss: 1.9465 - acc: 0.6452    \n",
      "Epoch 30/35\n",
      "189/188 [==============================] - 12s - loss: 1.9084 - acc: 0.6647    \n",
      "Epoch 31/35\n",
      "189/188 [==============================] - 12s - loss: 1.8494 - acc: 0.6812    \n",
      "Epoch 32/35\n",
      "189/188 [==============================] - 12s - loss: 1.8147 - acc: 0.6947    \n",
      "Epoch 33/35\n",
      "189/188 [==============================] - 12s - loss: 1.7723 - acc: 0.7104    \n",
      "Epoch 34/35\n",
      "189/188 [==============================] - 12s - loss: 1.7335 - acc: 0.7168    \n",
      "Epoch 35/35\n",
      "189/188 [==============================] - 12s - loss: 1.6916 - acc: 0.7295    \n",
      "Accuracy = 0.307431158054\n",
      "Time = 439.4215052127838\n",
      "Epoch 1/35\n",
      "189/188 [==============================] - 12s - loss: 4.2420 - acc: 0.1381    \n",
      "Epoch 2/35\n",
      "189/188 [==============================] - 12s - loss: 3.8877 - acc: 0.2094    \n",
      "Epoch 3/35\n",
      "189/188 [==============================] - 12s - loss: 3.7133 - acc: 0.2368    \n",
      "Epoch 4/35\n",
      "189/188 [==============================] - 12s - loss: 3.5646 - acc: 0.2596    \n",
      "Epoch 5/35\n",
      "189/188 [==============================] - 12s - loss: 3.4496 - acc: 0.2735    \n",
      "Epoch 6/35\n",
      "189/188 [==============================] - 12s - loss: 3.3489 - acc: 0.2838    \n",
      "Epoch 7/35\n",
      "189/188 [==============================] - 12s - loss: 3.2532 - acc: 0.2979    \n",
      "Epoch 8/35\n",
      "189/188 [==============================] - 12s - loss: 3.1648 - acc: 0.3147    \n",
      "Epoch 9/35\n",
      "189/188 [==============================] - 12s - loss: 3.0914 - acc: 0.3217    \n",
      "Epoch 10/35\n",
      "189/188 [==============================] - 11s - loss: 3.0220 - acc: 0.3344    \n",
      "Epoch 11/35\n",
      "189/188 [==============================] - 12s - loss: 2.9405 - acc: 0.3527    \n",
      "Epoch 12/35\n",
      "189/188 [==============================] - 12s - loss: 2.8759 - acc: 0.3679    \n",
      "Epoch 13/35\n",
      "189/188 [==============================] - 13s - loss: 2.8113 - acc: 0.3837    \n",
      "Epoch 14/35\n",
      "189/188 [==============================] - 12s - loss: 2.7480 - acc: 0.4025    \n",
      "Epoch 15/35\n",
      "189/188 [==============================] - 12s - loss: 2.6828 - acc: 0.4157    \n",
      "Epoch 16/35\n",
      "189/188 [==============================] - 12s - loss: 2.6255 - acc: 0.4342    \n",
      "Epoch 17/35\n",
      "189/188 [==============================] - 12s - loss: 2.5674 - acc: 0.4515    \n",
      "Epoch 18/35\n",
      "189/188 [==============================] - 12s - loss: 2.5167 - acc: 0.4671    \n",
      "Epoch 19/35\n",
      "189/188 [==============================] - 12s - loss: 2.4573 - acc: 0.4796    \n",
      "Epoch 20/35\n",
      "189/188 [==============================] - 12s - loss: 2.4056 - acc: 0.4995    \n",
      "Epoch 21/35\n",
      "189/188 [==============================] - 12s - loss: 2.3531 - acc: 0.5179    \n",
      "Epoch 22/35\n",
      "189/188 [==============================] - 13s - loss: 2.3062 - acc: 0.5298    \n",
      "Epoch 23/35\n",
      "189/188 [==============================] - 13s - loss: 2.2532 - acc: 0.5490    \n",
      "Epoch 24/35\n",
      "189/188 [==============================] - 12s - loss: 2.2064 - acc: 0.5648    \n",
      "Epoch 25/35\n",
      "189/188 [==============================] - 13s - loss: 2.1587 - acc: 0.5842    \n",
      "Epoch 26/35\n",
      "189/188 [==============================] - 12s - loss: 2.1055 - acc: 0.5987    \n",
      "Epoch 27/35\n",
      "189/188 [==============================] - 12s - loss: 2.0713 - acc: 0.6098    \n",
      "Epoch 28/35\n",
      "189/188 [==============================] - 13s - loss: 2.0093 - acc: 0.6339    \n",
      "Epoch 29/35\n",
      "189/188 [==============================] - 12s - loss: 1.9818 - acc: 0.6384    \n",
      "Epoch 30/35\n",
      "189/188 [==============================] - 12s - loss: 1.9358 - acc: 0.6602    \n",
      "Epoch 31/35\n",
      "189/188 [==============================] - 12s - loss: 1.8903 - acc: 0.6760    \n",
      "Epoch 32/35\n",
      "189/188 [==============================] - 12s - loss: 1.8456 - acc: 0.6881    \n",
      "Epoch 33/35\n",
      "189/188 [==============================] - 13s - loss: 1.8129 - acc: 0.6999    \n",
      "Epoch 34/35\n",
      "189/188 [==============================] - 12s - loss: 1.7650 - acc: 0.7089    \n",
      "Epoch 35/35\n",
      "189/188 [==============================] - 12s - loss: 1.7354 - acc: 0.7197    \n",
      "Accuracy = 0.308940022633\n",
      "Time = 445.4868619441986\n",
      "Epoch 1/35\n",
      "189/188 [==============================] - 13s - loss: 4.2656 - acc: 0.1359    \n",
      "Epoch 2/35\n",
      "189/188 [==============================] - 12s - loss: 3.9227 - acc: 0.2006    \n",
      "Epoch 3/35\n",
      "189/188 [==============================] - 12s - loss: 3.7433 - acc: 0.2299    \n",
      "Epoch 4/35\n",
      "189/188 [==============================] - 13s - loss: 3.6094 - acc: 0.2526    \n",
      "Epoch 5/35\n",
      "189/188 [==============================] - 12s - loss: 3.5068 - acc: 0.2649    \n",
      "Epoch 6/35\n",
      "189/188 [==============================] - 12s - loss: 3.3986 - acc: 0.2810    \n",
      "Epoch 7/35\n",
      "189/188 [==============================] - 12s - loss: 3.3033 - acc: 0.2960    \n",
      "Epoch 8/35\n",
      "189/188 [==============================] - 12s - loss: 3.2191 - acc: 0.3067    \n",
      "Epoch 9/35\n",
      "189/188 [==============================] - 12s - loss: 3.1390 - acc: 0.3168    \n",
      "Epoch 10/35\n",
      "189/188 [==============================] - 13s - loss: 3.0635 - acc: 0.3321    \n",
      "Epoch 11/35\n",
      "189/188 [==============================] - 12s - loss: 2.9967 - acc: 0.3464    \n",
      "Epoch 12/35\n",
      "189/188 [==============================] - 12s - loss: 2.9311 - acc: 0.3594    \n",
      "Epoch 13/35\n",
      "189/188 [==============================] - 12s - loss: 2.8618 - acc: 0.3724    \n",
      "Epoch 14/35\n",
      "189/188 [==============================] - 12s - loss: 2.8026 - acc: 0.3911    \n",
      "Epoch 15/35\n",
      "189/188 [==============================] - 12s - loss: 2.7473 - acc: 0.4000    \n",
      "Epoch 16/35\n",
      "189/188 [==============================] - 12s - loss: 2.6836 - acc: 0.4181    \n",
      "Epoch 17/35\n",
      "189/188 [==============================] - 12s - loss: 2.6284 - acc: 0.4345    \n",
      "Epoch 18/35\n",
      "189/188 [==============================] - 12s - loss: 2.5702 - acc: 0.4464    \n",
      "Epoch 19/35\n",
      "189/188 [==============================] - 12s - loss: 2.5203 - acc: 0.4632    \n",
      "Epoch 20/35\n",
      "189/188 [==============================] - 12s - loss: 2.4660 - acc: 0.4740    \n",
      "Epoch 21/35\n",
      "189/188 [==============================] - 12s - loss: 2.4150 - acc: 0.4939    \n",
      "Epoch 22/35\n",
      "189/188 [==============================] - 12s - loss: 2.3618 - acc: 0.5079    \n",
      "Epoch 23/35\n",
      "189/188 [==============================] - 12s - loss: 2.3105 - acc: 0.5267    \n",
      "Epoch 24/35\n",
      "189/188 [==============================] - 12s - loss: 2.2698 - acc: 0.5397    \n",
      "Epoch 25/35\n",
      "189/188 [==============================] - 12s - loss: 2.2189 - acc: 0.5588    \n",
      "Epoch 26/35\n",
      "189/188 [==============================] - 12s - loss: 2.1751 - acc: 0.5744    \n",
      "Epoch 27/35\n",
      "189/188 [==============================] - 12s - loss: 2.1407 - acc: 0.5849    \n",
      "Epoch 28/35\n",
      "189/188 [==============================] - 12s - loss: 2.0897 - acc: 0.5992    \n",
      "Epoch 29/35\n",
      "189/188 [==============================] - 12s - loss: 2.0478 - acc: 0.6122    \n",
      "Epoch 30/35\n",
      "189/188 [==============================] - 12s - loss: 2.0052 - acc: 0.6347    \n",
      "Epoch 31/35\n",
      "189/188 [==============================] - 12s - loss: 1.9553 - acc: 0.6515    \n",
      "Epoch 32/35\n",
      "189/188 [==============================] - 12s - loss: 1.9191 - acc: 0.6667    \n",
      "Epoch 33/35\n",
      "189/188 [==============================] - 12s - loss: 1.8857 - acc: 0.6749    \n",
      "Epoch 34/35\n",
      "189/188 [==============================] - 12s - loss: 1.8438 - acc: 0.6866    \n",
      "Epoch 35/35\n",
      "189/188 [==============================] - 12s - loss: 1.7988 - acc: 0.7016    \n",
      "Accuracy = 0.31648434553\n",
      "Time = 438.9541029930115\n",
      "Epoch 1/35\n",
      "189/188 [==============================] - 12s - loss: 4.2603 - acc: 0.1421    \n",
      "Epoch 2/35\n",
      "189/188 [==============================] - 11s - loss: 3.9132 - acc: 0.2022    \n",
      "Epoch 3/35\n",
      "189/188 [==============================] - 11s - loss: 3.7565 - acc: 0.2311    \n",
      "Epoch 4/35\n",
      "189/188 [==============================] - 11s - loss: 3.6292 - acc: 0.2483    \n",
      "Epoch 5/35\n",
      "189/188 [==============================] - 11s - loss: 3.5167 - acc: 0.2639    \n",
      "Epoch 6/35\n",
      "189/188 [==============================] - 11s - loss: 3.4217 - acc: 0.2765    \n",
      "Epoch 7/35\n",
      "189/188 [==============================] - 11s - loss: 3.3227 - acc: 0.2926    \n",
      "Epoch 8/35\n",
      "189/188 [==============================] - 11s - loss: 3.2330 - acc: 0.3036    \n",
      "Epoch 9/35\n",
      "189/188 [==============================] - 12s - loss: 3.1787 - acc: 0.3067    \n",
      "Epoch 10/35\n",
      "189/188 [==============================] - 12s - loss: 3.0921 - acc: 0.3227    \n",
      "Epoch 11/35\n",
      "189/188 [==============================] - 12s - loss: 3.0089 - acc: 0.3416    \n",
      "Epoch 12/35\n",
      "189/188 [==============================] - 12s - loss: 2.9550 - acc: 0.3508    \n",
      "Epoch 13/35\n",
      "189/188 [==============================] - 12s - loss: 2.8843 - acc: 0.3652    \n",
      "Epoch 14/35\n",
      "189/188 [==============================] - 12s - loss: 2.8231 - acc: 0.3842    \n",
      "Epoch 15/35\n",
      "189/188 [==============================] - 12s - loss: 2.7613 - acc: 0.4026    \n",
      "Epoch 16/35\n",
      "189/188 [==============================] - 12s - loss: 2.6949 - acc: 0.4173    \n",
      "Epoch 17/35\n",
      "189/188 [==============================] - 12s - loss: 2.6472 - acc: 0.4317    \n",
      "Epoch 18/35\n",
      "189/188 [==============================] - 12s - loss: 2.5875 - acc: 0.4441    \n",
      "Epoch 19/35\n",
      "189/188 [==============================] - 12s - loss: 2.5291 - acc: 0.4633    \n",
      "Epoch 20/35\n",
      "189/188 [==============================] - 12s - loss: 2.4935 - acc: 0.4738    \n",
      "Epoch 21/35\n",
      "189/188 [==============================] - 12s - loss: 2.4291 - acc: 0.4953    \n",
      "Epoch 22/35\n",
      "189/188 [==============================] - 12s - loss: 2.3717 - acc: 0.5087    \n",
      "Epoch 23/35\n",
      "189/188 [==============================] - 12s - loss: 2.3302 - acc: 0.5227    \n",
      "Epoch 24/35\n",
      "189/188 [==============================] - 12s - loss: 2.2787 - acc: 0.5389    \n",
      "Epoch 25/35\n",
      "189/188 [==============================] - 12s - loss: 2.2373 - acc: 0.5550    \n",
      "Epoch 26/35\n",
      "189/188 [==============================] - 12s - loss: 2.1873 - acc: 0.5654    \n",
      "Epoch 27/35\n",
      "189/188 [==============================] - 12s - loss: 2.1432 - acc: 0.5854    \n",
      "Epoch 28/35\n",
      "189/188 [==============================] - 12s - loss: 2.0976 - acc: 0.5958    \n",
      "Epoch 29/35\n",
      "189/188 [==============================] - 12s - loss: 2.0504 - acc: 0.6158    \n",
      "Epoch 30/35\n",
      "189/188 [==============================] - 12s - loss: 2.0049 - acc: 0.6290    \n",
      "Epoch 31/35\n",
      "189/188 [==============================] - 12s - loss: 1.9639 - acc: 0.6544    \n",
      "Epoch 32/35\n",
      "189/188 [==============================] - 12s - loss: 1.9323 - acc: 0.6576    \n",
      "Epoch 33/35\n",
      "189/188 [==============================] - 12s - loss: 1.8829 - acc: 0.6797    \n",
      "Epoch 34/35\n",
      "189/188 [==============================] - 12s - loss: 1.8474 - acc: 0.6836    \n",
      "Epoch 35/35\n",
      "189/188 [==============================] - 12s - loss: 1.8054 - acc: 0.7005    \n",
      "Accuracy = 0.321388155413\n",
      "Time = 432.6826202869415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "189/188 [==============================] - 12s - loss: 4.2809 - acc: 0.1293    \n",
      "Epoch 2/35\n",
      "189/188 [==============================] - 12s - loss: 3.9301 - acc: 0.2032    \n",
      "Epoch 3/35\n",
      "189/188 [==============================] - 12s - loss: 3.7658 - acc: 0.2261    \n",
      "Epoch 4/35\n",
      "189/188 [==============================] - 12s - loss: 3.6244 - acc: 0.2473    \n",
      "Epoch 5/35\n",
      "189/188 [==============================] - 12s - loss: 3.5043 - acc: 0.2647    \n",
      "Epoch 6/35\n",
      "189/188 [==============================] - 12s - loss: 3.4073 - acc: 0.2810    \n",
      "Epoch 7/35\n",
      "189/188 [==============================] - 12s - loss: 3.3060 - acc: 0.2936    \n",
      "Epoch 8/35\n",
      "189/188 [==============================] - 12s - loss: 3.2377 - acc: 0.3027    \n",
      "Epoch 9/35\n",
      "189/188 [==============================] - 12s - loss: 3.1512 - acc: 0.3212    \n",
      "Epoch 10/35\n",
      "189/188 [==============================] - 12s - loss: 3.0881 - acc: 0.3294    \n",
      "Epoch 11/35\n",
      "189/188 [==============================] - 12s - loss: 2.9948 - acc: 0.3493    \n",
      "Epoch 12/35\n",
      "189/188 [==============================] - 12s - loss: 2.9429 - acc: 0.3606    \n",
      "Epoch 13/35\n",
      "189/188 [==============================] - 12s - loss: 2.8703 - acc: 0.3776    \n",
      "Epoch 14/35\n",
      "189/188 [==============================] - 12s - loss: 2.8028 - acc: 0.3946    \n",
      "Epoch 15/35\n",
      "189/188 [==============================] - 12s - loss: 2.7525 - acc: 0.4044    \n",
      "Epoch 16/35\n",
      "189/188 [==============================] - 12s - loss: 2.7072 - acc: 0.4149    \n",
      "Epoch 17/35\n",
      "189/188 [==============================] - 12s - loss: 2.6287 - acc: 0.4331    \n",
      "Epoch 18/35\n",
      "189/188 [==============================] - 12s - loss: 2.5826 - acc: 0.4439    \n",
      "Epoch 19/35\n",
      "189/188 [==============================] - 12s - loss: 2.5211 - acc: 0.4680    \n",
      "Epoch 20/35\n",
      "189/188 [==============================] - 12s - loss: 2.4816 - acc: 0.4730    \n",
      "Epoch 21/35\n",
      "189/188 [==============================] - 12s - loss: 2.4224 - acc: 0.4960    \n",
      "Epoch 22/35\n",
      "189/188 [==============================] - 12s - loss: 2.3708 - acc: 0.5055    \n",
      "Epoch 23/35\n",
      "189/188 [==============================] - 12s - loss: 2.3112 - acc: 0.5252    \n",
      "Epoch 24/35\n",
      "189/188 [==============================] - 12s - loss: 2.2783 - acc: 0.5425    \n",
      "Epoch 25/35\n",
      "189/188 [==============================] - 12s - loss: 2.2220 - acc: 0.5582    \n",
      "Epoch 26/35\n",
      "189/188 [==============================] - 12s - loss: 2.1778 - acc: 0.5701    \n",
      "Epoch 27/35\n",
      "189/188 [==============================] - 12s - loss: 2.1367 - acc: 0.5862    \n",
      "Epoch 28/35\n",
      "189/188 [==============================] - 12s - loss: 2.0939 - acc: 0.5974    \n",
      "Epoch 29/35\n",
      "189/188 [==============================] - 12s - loss: 2.0440 - acc: 0.6202    \n",
      "Epoch 30/35\n",
      "189/188 [==============================] - 12s - loss: 2.0071 - acc: 0.6310    \n",
      "Epoch 31/35\n",
      "189/188 [==============================] - 12s - loss: 1.9563 - acc: 0.6451    \n",
      "Epoch 32/35\n",
      "189/188 [==============================] - 12s - loss: 1.9160 - acc: 0.6595    \n",
      "Epoch 33/35\n",
      "189/188 [==============================] - 12s - loss: 1.8719 - acc: 0.6822    \n",
      "Epoch 34/35\n",
      "189/188 [==============================] - 12s - loss: 1.8486 - acc: 0.6854    \n",
      "Epoch 35/35\n",
      "189/188 [==============================] - 12s - loss: 1.7979 - acc: 0.7024    \n",
      "Accuracy = 0.324028668427\n",
      "Time = 432.3795425891876\n"
     ]
    }
   ],
   "source": [
    "model_1 = Sequential()\n",
    "model_1.add(Flatten(input_shape=(128,128,3),name='flatten'))\n",
    "model_1.add(Dense(units=300, activation='sigmoid', name='dens_sigmoid_1'))\n",
    "model_1.add(Dense(units=101, activation='softmax'))\n",
    "model_1.load_weights('DenseAutoEncoderNET1_weights_1', by_name=True)\n",
    "model_1.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "#%%\n",
    "early_stopping=EarlyStopping(monitor='acc', patience=3, verbose=0, mode='auto')\n",
    "t0=time.time()\n",
    "model_1.fit_generator(train_generator,\n",
    "        steps_per_epoch=train_count/batch_size,\n",
    "        epochs=epochs,\n",
    "        callbacks=[early_stopping])\n",
    "t1=time.time()\n",
    "loss_and_metrics = model_1.evaluate_generator(test_generator, steps=test_count/batch_size)\n",
    "print('Accuracy =',loss_and_metrics[1])\n",
    "print('Time =',(t1-t0))\n",
    "#%%\n",
    "\n",
    "model_2 = Sequential()\n",
    "model_2.add(Flatten(input_shape=(128,128,3),name='flatten'))\n",
    "model_2.add(Dense(units=300, activation='sigmoid', name='dens_sigmoid_1'))\n",
    "model_2.add(Dense(units=101, activation='softmax'))\n",
    "model_2.load_weights('DenseAutoEncoderNET1_weights_2', by_name=True)\n",
    "model_2.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "#%%\n",
    "early_stopping=EarlyStopping(monitor='acc', patience=3, verbose=0, mode='auto')\n",
    "t0=time.time()\n",
    "model_2.fit_generator(train_generator,\n",
    "        steps_per_epoch=train_count/batch_size,\n",
    "        epochs=epochs,\n",
    "        callbacks=[early_stopping])\n",
    "t1=time.time()\n",
    "loss_and_metrics = model_2.evaluate_generator(test_generator, steps=test_count/batch_size)\n",
    "print('Accuracy =',loss_and_metrics[1])\n",
    "print('Time =',(t1-t0))\n",
    "#%%\n",
    "\n",
    "model_3 = Sequential()\n",
    "model_3.add(Flatten(input_shape=(128,128,3),name='flatten'))\n",
    "model_3.add(Dense(units=300, activation='sigmoid', name='dens_sigmoid_1'))\n",
    "model_3.add(Dense(units=101, activation='softmax'))\n",
    "model_3.load_weights('DenseAutoEncoderNET1_weights_3', by_name=True)\n",
    "model_3.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "#%%\n",
    "early_stopping=EarlyStopping(monitor='acc', patience=3, verbose=0, mode='auto')\n",
    "t0=time.time()\n",
    "model_3.fit_generator(train_generator,\n",
    "        steps_per_epoch=train_count/batch_size,\n",
    "        epochs=epochs,\n",
    "        callbacks=[early_stopping])\n",
    "t1=time.time()\n",
    "loss_and_metrics = model_3.evaluate_generator(test_generator, steps=test_count/batch_size)\n",
    "print('Accuracy =',loss_and_metrics[1])\n",
    "print('Time =',(t1-t0))\n",
    "#%%\n",
    "\n",
    "model_4 = Sequential()\n",
    "model_4.add(Flatten(input_shape=(128,128,3),name='flatten'))\n",
    "model_4.add(Dense(units=300, activation='sigmoid', name='dens_sigmoid_1'))\n",
    "model_4.add(Dense(units=101, activation='softmax'))\n",
    "model_4.load_weights('DenseAutoEncoderNET1_weights_4', by_name=True)\n",
    "model_4.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "#%%\n",
    "early_stopping=EarlyStopping(monitor='acc', patience=3, verbose=0, mode='auto')\n",
    "t0=time.time()\n",
    "model_4.fit_generator(train_generator,\n",
    "        steps_per_epoch=train_count/batch_size,\n",
    "        epochs=epochs,\n",
    "        callbacks=[early_stopping])\n",
    "t1=time.time()\n",
    "loss_and_metrics = model_4.evaluate_generator(test_generator, steps=test_count/batch_size)\n",
    "print('Accuracy =',loss_and_metrics[1])\n",
    "print('Time =',(t1-t0))\n",
    "#%%\n",
    "\n",
    "model_5 = Sequential()\n",
    "model_5.add(Flatten(input_shape=(128,128,3),name='flatten'))\n",
    "model_5.add(Dense(units=300, activation='sigmoid', name='dens_sigmoid_1'))\n",
    "model_5.add(Dense(units=101, activation='softmax'))\n",
    "model_5.load_weights('DenseAutoEncoderNET1_weights_5', by_name=True)\n",
    "model_5.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "#%%\n",
    "early_stopping=EarlyStopping(monitor='acc', patience=3, verbose=0, mode='auto')\n",
    "t0=time.time()\n",
    "model_5.fit_generator(train_generator,\n",
    "        steps_per_epoch=train_count/batch_size,\n",
    "        epochs=epochs,\n",
    "        callbacks=[early_stopping])\n",
    "t1=time.time()\n",
    "loss_and_metrics = model_5.evaluate_generator(test_generator, steps=test_count/batch_size)\n",
    "print('Accuracy =',loss_and_metrics[1])\n",
    "print('Time =',(t1-t0))\n",
    "\n",
    "#%%\n",
    "model_6 = Sequential()\n",
    "model_6.add(Flatten(input_shape=(128,128,3),name='flatten'))\n",
    "model_6.add(Dense(units=300, activation='sigmoid', name='dens_sigmoid_1'))\n",
    "model_6.add(Dense(units=101, activation='softmax'))\n",
    "model_6.load_weights('DenseAutoEncoderNET1_weights_6', by_name=True)\n",
    "model_6.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "#%%\n",
    "early_stopping=EarlyStopping(monitor='acc', patience=3, verbose=0, mode='auto')\n",
    "t0=time.time()\n",
    "model_6.fit_generator(train_generator,\n",
    "        steps_per_epoch=train_count/batch_size,\n",
    "        epochs=epochs,\n",
    "        callbacks=[early_stopping])\n",
    "t1=time.time()\n",
    "loss_and_metrics = model_6.evaluate_generator(test_generator, steps=test_count/batch_size)\n",
    "print('Accuracy =',loss_and_metrics[1])\n",
    "print('Time =',(t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "K.clear_session()\n",
    "sess = tf.Session()\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense NET2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 128, 128, 3)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 49152)             0         \n",
      "_________________________________________________________________\n",
      "dens_sigmoid_1 (Dense)       (None, 1000)              49153000  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 49152)             49201152  \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 128, 128, 3)       0         \n",
      "=================================================================\n",
      "Total params: 98,354,152\n",
      "Trainable params: 98,354,152\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/35\n",
      "189/188 [==============================] - 19s - loss: 0.1021    \n",
      "Epoch 2/35\n",
      "189/188 [==============================] - 18s - loss: 0.1024    \n",
      "Epoch 3/35\n",
      "189/188 [==============================] - 18s - loss: 0.1004    \n",
      "Epoch 4/35\n",
      "189/188 [==============================] - 18s - loss: 0.1002    \n",
      "Epoch 5/35\n",
      "189/188 [==============================] - 18s - loss: 0.0983    \n",
      "Epoch 6/35\n",
      "189/188 [==============================] - 18s - loss: 0.0980    \n",
      "Epoch 7/35\n",
      "189/188 [==============================] - 18s - loss: 0.0961    \n",
      "Epoch 8/35\n",
      "189/188 [==============================] - 18s - loss: 0.0965    \n",
      "Epoch 9/35\n",
      "189/188 [==============================] - 18s - loss: 0.0928    \n",
      "Epoch 10/35\n",
      "189/188 [==============================] - 18s - loss: 0.0932    \n",
      "Epoch 11/35\n",
      "189/188 [==============================] - 18s - loss: 0.0913    \n",
      "Epoch 12/35\n",
      "189/188 [==============================] - 18s - loss: 0.0913    \n",
      "Epoch 13/35\n",
      "189/188 [==============================] - 18s - loss: 0.0893    \n",
      "Epoch 14/35\n",
      "189/188 [==============================] - 18s - loss: 0.0884    \n",
      "Epoch 15/35\n",
      "189/188 [==============================] - 18s - loss: 0.0871    \n",
      "Epoch 16/35\n",
      "189/188 [==============================] - 18s - loss: 0.0870    \n",
      "Epoch 17/35\n",
      "189/188 [==============================] - 18s - loss: 0.0856    \n",
      "Epoch 18/35\n",
      "189/188 [==============================] - 18s - loss: 0.0849    \n",
      "Epoch 19/35\n",
      "189/188 [==============================] - 18s - loss: 0.0837    \n",
      "Epoch 20/35\n",
      "189/188 [==============================] - 18s - loss: 0.0838    \n",
      "Epoch 21/35\n",
      "189/188 [==============================] - 18s - loss: 0.0833    \n",
      "Epoch 22/35\n",
      "189/188 [==============================] - 18s - loss: 0.0823    \n",
      "Epoch 23/35\n",
      "189/188 [==============================] - 18s - loss: 0.0821    \n",
      "Epoch 24/35\n",
      "189/188 [==============================] - 18s - loss: 0.0814    \n",
      "Epoch 25/35\n",
      "189/188 [==============================] - 18s - loss: 0.0809    \n",
      "Epoch 26/35\n",
      "189/188 [==============================] - 18s - loss: 0.0811    \n",
      "Epoch 27/35\n",
      "189/188 [==============================] - 18s - loss: 0.0802    \n",
      "Epoch 28/35\n",
      "189/188 [==============================] - 18s - loss: 0.0807    \n",
      "Epoch 29/35\n",
      "189/188 [==============================] - 18s - loss: 0.0797    \n",
      "Epoch 30/35\n",
      "189/188 [==============================] - 18s - loss: 0.0793    \n",
      "Epoch 31/35\n",
      "189/188 [==============================] - 18s - loss: 0.0797    \n",
      "Epoch 32/35\n",
      "189/188 [==============================] - 18s - loss: 0.0792    \n",
      "Epoch 33/35\n",
      "189/188 [==============================] - 18s - loss: 0.0791    \n",
      "Epoch 34/35\n",
      "189/188 [==============================] - 18s - loss: 0.0788    \n",
      "Epoch 35/35\n",
      "189/188 [==============================] - 18s - loss: 0.0785    \n",
      "(6026, 1000)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dens_tanh_1 (Dense)          (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1000)              501000    \n",
      "=================================================================\n",
      "Total params: 1,001,500\n",
      "Trainable params: 1,001,500\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/35\n",
      "6026/6026 [==============================] - 1s - loss: 0.0287     \n",
      "Epoch 2/35\n",
      "6026/6026 [==============================] - 0s - loss: 0.0108     \n",
      "Epoch 3/35\n",
      "6026/6026 [==============================] - 0s - loss: 0.0088     \n",
      "Epoch 4/35\n",
      "6026/6026 [==============================] - 0s - loss: 0.0078     \n",
      "Epoch 5/35\n",
      "6026/6026 [==============================] - 0s - loss: 0.0071     \n",
      "Epoch 6/35\n",
      "6026/6026 [==============================] - 0s - loss: 0.0067     \n",
      "Epoch 7/35\n",
      "6026/6026 [==============================] - 0s - loss: 0.0063     \n",
      "Epoch 8/35\n",
      "6026/6026 [==============================] - 0s - loss: 0.0059     \n",
      "Epoch 9/35\n",
      "6026/6026 [==============================] - 0s - loss: 0.0056     \n",
      "Epoch 10/35\n",
      "6026/6026 [==============================] - 0s - loss: 0.0054     \n",
      "Epoch 11/35\n",
      "6026/6026 [==============================] - 1s - loss: 0.0052     \n",
      "Epoch 12/35\n",
      "6026/6026 [==============================] - 0s - loss: 0.0050     \n",
      "Epoch 13/35\n",
      "6026/6026 [==============================] - 0s - loss: 0.0048     \n",
      "Epoch 14/35\n",
      "6026/6026 [==============================] - 0s - loss: 0.0047     \n",
      "Epoch 15/35\n",
      "6026/6026 [==============================] - 0s - loss: 0.0046     \n",
      "Epoch 16/35\n",
      "6026/6026 [==============================] - 0s - loss: 0.0045     \n",
      "Epoch 17/35\n",
      "6026/6026 [==============================] - 0s - loss: 0.0044     \n",
      "Epoch 18/35\n",
      "6026/6026 [==============================] - 0s - loss: 0.0043     \n",
      "Epoch 19/35\n",
      "6026/6026 [==============================] - 0s - loss: 0.0042     \n",
      "Epoch 20/35\n",
      "6026/6026 [==============================] - 0s - loss: 0.0041     \n",
      "Epoch 21/35\n",
      "6026/6026 [==============================] - 0s - loss: 0.0040     \n",
      "Epoch 22/35\n",
      "6026/6026 [==============================] - 0s - loss: 0.0040     \n",
      "Epoch 23/35\n",
      "6026/6026 [==============================] - 0s - loss: 0.0039     \n",
      "Epoch 24/35\n",
      "6026/6026 [==============================] - 0s - loss: 0.0039     \n",
      "Epoch 25/35\n",
      "6026/6026 [==============================] - 0s - loss: 0.0038     \n",
      "Epoch 26/35\n",
      "6026/6026 [==============================] - 0s - loss: 0.0037     \n",
      "Epoch 27/35\n",
      "6026/6026 [==============================] - 0s - loss: 0.0037     \n",
      "Epoch 28/35\n",
      "6026/6026 [==============================] - 0s - loss: 0.0036     \n",
      "Epoch 29/35\n",
      "6026/6026 [==============================] - 0s - loss: 0.0036     \n",
      "Epoch 30/35\n",
      "6026/6026 [==============================] - 0s - loss: 0.0036     \n",
      "Epoch 31/35\n",
      "6026/6026 [==============================] - 0s - loss: 0.0035     \n",
      "Epoch 32/35\n",
      "6026/6026 [==============================] - 0s - loss: 0.0035     \n",
      "Epoch 33/35\n",
      "6026/6026 [==============================] - 0s - loss: 0.0034     \n",
      "Epoch 34/35\n",
      "6026/6026 [==============================] - 0s - loss: 0.0034     \n",
      "Epoch 35/35\n",
      "6026/6026 [==============================] - 0s - loss: 0.0033     \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dens_relu_1 (Dense)          (None, 300)               150300    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 500)               150500    \n",
      "=================================================================\n",
      "Total params: 300,800\n",
      "Trainable params: 300,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/35\n",
      "6026/6026 [==============================] - 1s - loss: 0.2294     \n",
      "Epoch 2/35\n",
      "6026/6026 [==============================] - 0s - loss: 0.2157     \n",
      "Epoch 3/35\n",
      "6026/6026 [==============================] - 0s - loss: 0.2142     \n",
      "Epoch 4/35\n",
      "6026/6026 [==============================] - 0s - loss: 0.2135     \n",
      "Epoch 5/35\n",
      "6026/6026 [==============================] - 0s - loss: 0.2131     \n",
      "Epoch 6/35\n",
      "6026/6026 [==============================] - 0s - loss: 0.2127     \n",
      "Epoch 7/35\n",
      "6026/6026 [==============================] - 0s - loss: 0.2121     \n",
      "Epoch 8/35\n",
      "6026/6026 [==============================] - 0s - loss: 0.2102     \n",
      "Epoch 9/35\n",
      "6026/6026 [==============================] - 0s - loss: 0.2098     \n",
      "Epoch 10/35\n",
      "6026/6026 [==============================] - 0s - loss: 0.2097     \n",
      "Epoch 11/35\n",
      "6026/6026 [==============================] - 0s - loss: 0.2097     \n",
      "Epoch 12/35\n",
      "6026/6026 [==============================] - 0s - loss: 0.2095     \n",
      "Epoch 13/35\n",
      "6026/6026 [==============================] - 0s - loss: 0.2094     \n",
      "Epoch 14/35\n",
      "6026/6026 [==============================] - 0s - loss: 0.2094     \n",
      "Epoch 15/35\n",
      "6026/6026 [==============================] - 0s - loss: 0.2094     \n",
      "Epoch 16/35\n",
      "6026/6026 [==============================] - 0s - loss: 0.2093     \n",
      "Epoch 17/35\n",
      "6026/6026 [==============================] - 0s - loss: 0.2093     \n",
      "Epoch 18/35\n",
      "6026/6026 [==============================] - 0s - loss: 0.2093     \n",
      "Epoch 19/35\n",
      "6026/6026 [==============================] - 0s - loss: 0.2093     \n",
      "Epoch 20/35\n",
      "6026/6026 [==============================] - 0s - loss: 0.2093     \n",
      "Epoch 21/35\n",
      "6026/6026 [==============================] - 0s - loss: 0.2093     \n",
      "Epoch 22/35\n",
      "6026/6026 [==============================] - 0s - loss: 0.2092     \n",
      "Epoch 23/35\n",
      "6026/6026 [==============================] - 0s - loss: 0.2092     \n",
      "Epoch 24/35\n",
      "6026/6026 [==============================] - 0s - loss: 0.2092     \n",
      "Epoch 25/35\n",
      "6026/6026 [==============================] - 0s - loss: 0.2092     \n",
      "Epoch 26/35\n",
      "6026/6026 [==============================] - 0s - loss: 0.2092     \n",
      "Epoch 27/35\n",
      "6026/6026 [==============================] - 0s - loss: 0.2090     \n",
      "Epoch 28/35\n",
      "6026/6026 [==============================] - 0s - loss: 0.2073     \n",
      "Epoch 29/35\n",
      "6026/6026 [==============================] - 0s - loss: 0.2064     \n",
      "Epoch 30/35\n",
      "6026/6026 [==============================] - 0s - loss: 0.2054     \n",
      "Epoch 31/35\n",
      "6026/6026 [==============================] - 0s - loss: 0.2054     \n",
      "Epoch 32/35\n",
      "6026/6026 [==============================] - 0s - loss: 0.2054     \n",
      "Epoch 33/35\n",
      "6026/6026 [==============================] - 0s - loss: 0.2054     \n",
      "Epoch 34/35\n",
      "6026/6026 [==============================] - 0s - loss: 0.2054     \n",
      "Epoch 35/35\n",
      "6026/6026 [==============================] - 0s - loss: 0.2054     \n"
     ]
    }
   ],
   "source": [
    "# first layer's autoencoder\n",
    "input_img = Input(shape=(img_width, img_height, 3))\n",
    "x = Flatten(input_shape=(img_width, img_height, 3))(input_img)\n",
    "encoded = (Dense(units=1000, activation='sigmoid', name='dens_sigmoid_1'))(x)\n",
    "x = Dense(img_width*img_height*3, activation='sigmoid')(encoded)\n",
    "decoded = Reshape((img_width, img_height, 3), input_shape=(img_width*img_height*3,))(x)\n",
    "\n",
    "autoencoder1 = Model(input_img, decoded)\n",
    "\n",
    "autoencoder1.compile(optimizer='adadelta', loss='mse')\n",
    "print (autoencoder1.summary())\n",
    "autoencoder1.fit_generator(\n",
    "        tuple_generator(pretrain_generator),\n",
    "        steps_per_epoch=train_samples/batch_size,\n",
    "        epochs=epochs\n",
    "        )\n",
    "plot_model(autoencoder1, to_file='DenseAutoEncoder2_l1.png', show_shapes=True, show_layer_names=True, rankdir='LR')\n",
    "autoencoder1.save('DenseAutoEncoderNET2_l1')\n",
    "autoencoder1.save_weights('DenseAutoEncoderNET2_weights_l1')\n",
    "\n",
    "encoder1 = Sequential()\n",
    "encoder1.add(Flatten(input_shape=(img_width, img_height, 3)))\n",
    "encoder1.add(Dense(units=1000, activation='sigmoid', name='dens_sigmoid_1'))\n",
    "encoder1.load_weights('DenseAutoEncoderNET2_weights_l1', by_name=True)\n",
    "encoder1.compile(optimizer='adadelta', loss='mse')\n",
    "encoder1_out = (encoder1.predict_generator(tuple_generator(pretrain_generator),steps=train_samples/batch_size))\n",
    "print (encoder1_out.shape)\n",
    "\n",
    "# second layer's autoencoder\n",
    "x = Input(shape=(1000,))\n",
    "encoded = (Dense(units=500, activation='tanh', name='dens_tanh_1'))(x)\n",
    "decoded = Dense(1000, activation='tanh')(encoded)\n",
    "\n",
    "autoencoder2 = Model(x, decoded)\n",
    "autoencoder2.compile(optimizer='adadelta', loss='mse')\n",
    "print (autoencoder2.summary())\n",
    "autoencoder2.fit(\n",
    "        encoder1_out, encoder1_out,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs\n",
    "        )\n",
    "plot_model(autoencoder2, to_file='DenseAutoEncoder2_l2.png', show_shapes=True, show_layer_names=True, rankdir='LR')\n",
    "autoencoder2.save('DenseAutoEncoderNET2_l2')\n",
    "autoencoder2.save_weights('DenseAutoEncoderNET2_weights_l2')\n",
    "\n",
    "encoder2 = Sequential()\n",
    "encoder2.add(Dense(units=500, activation='tanh', input_shape=(1000,), name='dens_tanh_1'))\n",
    "encoder2.load_weights('DenseAutoEncoderNET2_weights_l2', by_name=True)\n",
    "encoder2.compile(optimizer='adadelta', loss='mse')\n",
    "encoder2_out = encoder2.predict(encoder1_out)\n",
    "\n",
    "# third layer's autoencoder\n",
    "x = Input(shape=(500,))\n",
    "encoded = (Dense(units=300, activation='relu', name='dens_relu_1'))(x)\n",
    "decoded = Dense(500, activation='relu')(encoded)\n",
    "\n",
    "autoencoder3 = Model(x, decoded)\n",
    "autoencoder3.compile(optimizer='adadelta', loss='mse')\n",
    "print (autoencoder3.summary())\n",
    "autoencoder3.fit(\n",
    "        encoder2_out, encoder2_out,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs\n",
    "        )\n",
    "plot_model(autoencoder3, to_file='DenseAutoEncoder2_l3png', show_shapes=True, show_layer_names=True, rankdir='LR')\n",
    "autoencoder3.save('DenseAutoEncoderNET2_l3')\n",
    "autoencoder3.save_weights('DenseAutoEncoderNET2_weights_l3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "K.clear_session()\n",
    "sess = tf.Session()\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "189/188 [==============================] - 13s - loss: 3.8080 - acc: 0.2344    \n",
      "Epoch 2/35\n",
      "189/188 [==============================] - 12s - loss: 3.3514 - acc: 0.2878    \n",
      "Epoch 3/35\n",
      "189/188 [==============================] - 12s - loss: 3.0944 - acc: 0.3272    \n",
      "Epoch 4/35\n",
      "189/188 [==============================] - 12s - loss: 2.8729 - acc: 0.3690    \n",
      "Epoch 5/35\n",
      "189/188 [==============================] - 12s - loss: 2.6861 - acc: 0.4101    \n",
      "Epoch 6/35\n",
      "189/188 [==============================] - 12s - loss: 2.5309 - acc: 0.4459    \n",
      "Epoch 7/35\n",
      "189/188 [==============================] - 12s - loss: 2.3497 - acc: 0.4869    \n",
      "Epoch 8/35\n",
      "189/188 [==============================] - 12s - loss: 2.2022 - acc: 0.5329    \n",
      "Epoch 9/35\n",
      "189/188 [==============================] - 12s - loss: 2.0476 - acc: 0.5736    \n",
      "Epoch 10/35\n",
      "189/188 [==============================] - 12s - loss: 1.9202 - acc: 0.6076    \n",
      "Epoch 11/35\n",
      "189/188 [==============================] - 12s - loss: 1.7917 - acc: 0.6482    \n",
      "Epoch 12/35\n",
      "189/188 [==============================] - 12s - loss: 1.6675 - acc: 0.6835    \n",
      "Epoch 13/35\n",
      "189/188 [==============================] - 12s - loss: 1.5447 - acc: 0.7081    \n",
      "Epoch 14/35\n",
      "189/188 [==============================] - 12s - loss: 1.4239 - acc: 0.7372    \n",
      "Epoch 15/35\n",
      "189/188 [==============================] - 12s - loss: 1.3146 - acc: 0.7623    \n",
      "Epoch 16/35\n",
      "189/188 [==============================] - 12s - loss: 1.2196 - acc: 0.7829    \n",
      "Epoch 17/35\n",
      "189/188 [==============================] - 12s - loss: 1.1322 - acc: 0.8031    \n",
      "Epoch 18/35\n",
      "189/188 [==============================] - 12s - loss: 1.0412 - acc: 0.8237    \n",
      "Epoch 19/35\n",
      "189/188 [==============================] - 12s - loss: 0.9806 - acc: 0.8341    \n",
      "Epoch 20/35\n",
      "189/188 [==============================] - 12s - loss: 0.8971 - acc: 0.8521    \n",
      "Epoch 21/35\n",
      "189/188 [==============================] - 12s - loss: 0.8447 - acc: 0.8596    \n",
      "Epoch 22/35\n",
      "189/188 [==============================] - 12s - loss: 0.7677 - acc: 0.8773    \n",
      "Epoch 23/35\n",
      "189/188 [==============================] - 12s - loss: 0.7231 - acc: 0.8817    \n",
      "Epoch 24/35\n",
      "189/188 [==============================] - 12s - loss: 0.6694 - acc: 0.8908    \n",
      "Epoch 25/35\n",
      "189/188 [==============================] - 12s - loss: 0.6522 - acc: 0.8902    \n",
      "Epoch 26/35\n",
      "189/188 [==============================] - 12s - loss: 0.5937 - acc: 0.9054    \n",
      "Epoch 27/35\n",
      "189/188 [==============================] - 12s - loss: 0.5718 - acc: 0.9030    - E\n",
      "Epoch 28/35\n",
      "189/188 [==============================] - 12s - loss: 0.5312 - acc: 0.9132    \n",
      "Epoch 29/35\n",
      "189/188 [==============================] - 12s - loss: 0.5219 - acc: 0.9124    \n",
      "Epoch 30/35\n",
      "189/188 [==============================] - 12s - loss: 0.4869 - acc: 0.9196    \n",
      "Epoch 31/35\n",
      "189/188 [==============================] - 12s - loss: 0.4711 - acc: 0.9180    \n",
      "Epoch 32/35\n",
      "189/188 [==============================] - 12s - loss: 0.4476 - acc: 0.9216    \n",
      "Epoch 33/35\n",
      "189/188 [==============================] - 12s - loss: 0.4422 - acc: 0.9226    \n",
      "Epoch 34/35\n",
      "189/188 [==============================] - 12s - loss: 0.4198 - acc: 0.9281    \n",
      "Epoch 35/35\n",
      "189/188 [==============================] - 12s - loss: 0.3986 - acc: 0.9284    \n",
      "Accuracy = 0.349302150132\n",
      "Time = 440.41906237602234\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(128,128,3)))\n",
    "model.add(Dense(units=1000, activation='sigmoid', name='dens_sigmoid_1'))\n",
    "model.add(Dense(units=500, activation='tanh', name='dens_tanh_1'))\n",
    "model.add(Dense(units=300, activation='relu', name='dens_relu_1'))\n",
    "model.add(Dense(units=101, activation='softmax'))\n",
    "#model.load_weights('DenseAutoEncoderNET2_weights_l1', by_name=True)\n",
    "#model.load_weights('DenseAutoEncoderNET2_weights_l2', by_name=True)\n",
    "#model.load_weights('DenseAutoEncoderNET2_weights_l3', by_name=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "#%%\n",
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping=EarlyStopping(monitor='acc', patience=3, verbose=0, mode='auto')\n",
    "import time\n",
    "t0=time.time()\n",
    "model.fit_generator(train_generator,\n",
    "        steps_per_epoch=train_count/batch_size,\n",
    "        epochs=epochs,\n",
    "        callbacks=[early_stopping])\n",
    "t1=time.time()\n",
    "loss_and_metrics = model.evaluate_generator(test_generator, steps=test_count/batch_size)\n",
    "print('Accuracy =',loss_and_metrics[1])\n",
    "print('Time =',(t1-t0))\n",
    "#%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "K.clear_session()\n",
    "sess = tf.Session()\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "189/188 [==============================] - 13s - loss: 4.0606 - acc: 0.1783    \n",
      "Epoch 2/35\n",
      "189/188 [==============================] - 12s - loss: 3.6509 - acc: 0.2518    \n",
      "Epoch 3/35\n",
      "189/188 [==============================] - 12s - loss: 3.4014 - acc: 0.2857    \n",
      "Epoch 4/35\n",
      "189/188 [==============================] - 12s - loss: 3.2195 - acc: 0.3087    \n",
      "Epoch 5/35\n",
      "189/188 [==============================] - 12s - loss: 3.0472 - acc: 0.3385    \n",
      "Epoch 6/35\n",
      "189/188 [==============================] - 12s - loss: 2.9006 - acc: 0.3693    \n",
      "Epoch 7/35\n",
      "189/188 [==============================] - 12s - loss: 2.7526 - acc: 0.4034    \n",
      "Epoch 8/35\n",
      "189/188 [==============================] - 12s - loss: 2.6218 - acc: 0.4307    \n",
      "Epoch 9/35\n",
      "189/188 [==============================] - 12s - loss: 2.4786 - acc: 0.4597    \n",
      "Epoch 10/35\n",
      "189/188 [==============================] - 12s - loss: 2.3560 - acc: 0.4911    \n",
      "Epoch 11/35\n",
      "189/188 [==============================] - 12s - loss: 2.2476 - acc: 0.5209    \n",
      "Epoch 12/35\n",
      "189/188 [==============================] - 12s - loss: 2.1060 - acc: 0.5534    \n",
      "Epoch 13/35\n",
      "189/188 [==============================] - 12s - loss: 1.9982 - acc: 0.5896    \n",
      "Epoch 14/35\n",
      "189/188 [==============================] - 12s - loss: 1.8863 - acc: 0.6188    \n",
      "Epoch 15/35\n",
      "189/188 [==============================] - 12s - loss: 1.7640 - acc: 0.6531    \n",
      "Epoch 16/35\n",
      "189/188 [==============================] - 12s - loss: 1.6586 - acc: 0.6785    \n",
      "Epoch 17/35\n",
      "189/188 [==============================] - 12s - loss: 1.5494 - acc: 0.7097    \n",
      "Epoch 18/35\n",
      "189/188 [==============================] - 12s - loss: 1.4671 - acc: 0.7262    \n",
      "Epoch 19/35\n",
      "189/188 [==============================] - 12s - loss: 1.3494 - acc: 0.7555    - ETA: 1s - loss:\n",
      "Epoch 20/35\n",
      "189/188 [==============================] - 12s - loss: 1.2623 - acc: 0.7700    \n",
      "Epoch 21/35\n",
      "189/188 [==============================] - 12s - loss: 1.1592 - acc: 0.7961    \n",
      "Epoch 22/35\n",
      "189/188 [==============================] - 12s - loss: 1.0909 - acc: 0.8050    \n",
      "Epoch 23/35\n",
      "189/188 [==============================] - 12s - loss: 1.0084 - acc: 0.8256    \n",
      "Epoch 24/35\n",
      "189/188 [==============================] - 12s - loss: 1.0002 - acc: 0.8196    \n",
      "Epoch 25/35\n",
      "189/188 [==============================] - 12s - loss: 0.8944 - acc: 0.8432    \n",
      "Epoch 26/35\n",
      "189/188 [==============================] - 12s - loss: 0.8162 - acc: 0.8578    \n",
      "Epoch 27/35\n",
      "189/188 [==============================] - 12s - loss: 0.7609 - acc: 0.8637    \n",
      "Epoch 28/35\n",
      "189/188 [==============================] - 12s - loss: 0.7427 - acc: 0.8711    \n",
      "Epoch 29/35\n",
      "189/188 [==============================] - 12s - loss: 0.6920 - acc: 0.8786    \n",
      "Epoch 30/35\n",
      "189/188 [==============================] - 12s - loss: 0.6413 - acc: 0.8892    \n",
      "Epoch 31/35\n",
      "189/188 [==============================] - 12s - loss: 0.6066 - acc: 0.8938    \n",
      "Epoch 32/35\n",
      "189/188 [==============================] - 12s - loss: 0.5996 - acc: 0.8967    \n",
      "Epoch 33/35\n",
      "189/188 [==============================] - 12s - loss: 0.5463 - acc: 0.9074    - ETA: 0s - loss: 0.5424 - a\n",
      "Epoch 34/35\n",
      "189/188 [==============================] - 13s - loss: 0.5374 - acc: 0.9033    \n",
      "Epoch 35/35\n",
      "189/188 [==============================] - 13s - loss: 0.5073 - acc: 0.9110    \n",
      "Accuracy = 0.336476801207\n",
      "Time = 440.82030844688416\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(128,128,3)))\n",
    "model.add(Dense(units=1000, activation='sigmoid', name='dens_sigmoid_1'))\n",
    "model.add(Dense(units=500, activation='tanh', name='dens_tanh_1'))\n",
    "model.add(Dense(units=300, activation='relu', name='dens_relu_1'))\n",
    "model.add(Dense(units=101, activation='softmax'))\n",
    "model.load_weights('DenseAutoEncoderNET2_weights_l1', by_name=True)\n",
    "model.load_weights('DenseAutoEncoderNET2_weights_l2', by_name=True)\n",
    "model.load_weights('DenseAutoEncoderNET2_weights_l3', by_name=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "#%%\n",
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping=EarlyStopping(monitor='acc', patience=3, verbose=0, mode='auto')\n",
    "import time\n",
    "t0=time.time()\n",
    "model.fit_generator(train_generator,\n",
    "        steps_per_epoch=train_count/batch_size,\n",
    "        epochs=epochs,\n",
    "        callbacks=[early_stopping])\n",
    "t1=time.time()\n",
    "loss_and_metrics = model.evaluate_generator(test_generator, steps=test_count/batch_size)\n",
    "print('Accuracy =',loss_and_metrics[1])\n",
    "print('Time =',(t1-t0))\n",
    "#%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "K.clear_session()\n",
    "sess = tf.Session()\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6026 images belonging to 101 classes.\n",
      "Found 2651 images belonging to 101 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Input, Dense, Flatten, Reshape, Dropout, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import SGD\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "def tuple_generator(generator):\n",
    "    for batch in generator:\n",
    "        yield (batch, batch)\n",
    "\n",
    "np.random.seed(777)\n",
    "#if (len(sys.argv)<3):\n",
    "#    print(\"Input arguments:\")\n",
    "#    print(\"1. Train images path\")\n",
    "#    print(\"2. Test images path\")\n",
    "#    exit()\n",
    "PreTrainImagesPath=\"D:/SomeData/Caltech101/TrainImages\"\n",
    "PreTestImagesPath=\"D:/SomeData/Caltech101/TestImages\"\n",
    "TrainImagesPath=\"D:/SomeData/Caltech101/TrainImages\"\n",
    "TestImagesPath=\"D:/SomeData/Caltech101/TestImages\"\n",
    "img_width, img_height = 128, 128\n",
    "epochs = 35\n",
    "batch_size_1 = 16\n",
    "batch_size = 32\n",
    "batch_size_3 = 64\n",
    "dropout_rate = 0.3\n",
    "latent_dim = 101\n",
    "\n",
    "datagen=ImageDataGenerator(rescale=1./255)\n",
    "pretrain_generator = datagen.flow_from_directory(\n",
    "        PreTrainImagesPath,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None)\n",
    "train_samples = pretrain_generator.n\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "        PreTestImagesPath,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None)\n",
    "nb_validation_samples = validation_generator.n\n",
    "\n",
    "sgd_4 = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "sgd_5 = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "sgd_6 = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv NET1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1290: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 128, 128, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv_tanh_1 (Conv2D)         (None, 128, 128, 32)      896       \n",
      "_________________________________________________________________\n",
      "conv_tanh_2 (Conv2D)         (None, 128, 128, 32)      9248      \n",
      "_________________________________________________________________\n",
      "maxpool_1 (MaxPooling2D)     (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 128, 128, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 128, 128, 32)      9248      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 128, 128, 3)       867       \n",
      "=================================================================\n",
      "Total params: 20,259\n",
      "Trainable params: 20,259\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "189/188 [==============================] - 111s - loss: 0.0353   \n",
      "Epoch 2/10\n",
      "189/188 [==============================] - 11s - loss: 0.0160    \n",
      "Epoch 3/10\n",
      "189/188 [==============================] - 11s - loss: 0.0134    \n",
      "Epoch 4/10\n",
      "137/188 [====================>.........] - ETA: 3s - loss: 0.0120"
     ]
    }
   ],
   "source": [
    "# first layer's autoencoder\n",
    "input_img = Input(shape=(img_width, img_height, 3))\n",
    "x = Conv2D(32, (3, 3), activation='tanh', padding='same', input_shape=(img_width,img_height,3), name='conv_tanh_1')(input_img)\n",
    "x = Conv2D(32, (3, 3), activation='tanh', padding='same', name='conv_tanh_2')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same', name='maxpool_1')(x)\n",
    "encoded = Dropout((0.25), name='dropout_1')(x)\n",
    "x = UpSampling2D((2, 2))(encoded)\n",
    "x = Conv2D(32, (3, 3), activation='tanh', padding='same')(x)\n",
    "decoded = Conv2D(3, (3, 3), activation='tanh', padding='same')(x)\n",
    "\n",
    "autoencoder1 = Model(input_img, decoded)\n",
    "\n",
    "autoencoder1.compile(optimizer='adadelta', loss='mse')\n",
    "print (autoencoder1.summary())\n",
    "autoencoder1.fit_generator(\n",
    "        tuple_generator(pretrain_generator),\n",
    "        steps_per_epoch=train_samples/batch_size,\n",
    "        epochs=epochs\n",
    "        )\n",
    "plot_model(autoencoder1, to_file='ConvAutoEncoder2_l1.png', show_shapes=True, show_layer_names=True, rankdir='LR')\n",
    "autoencoder1.save('ConvAutoEncoderNET1_l1')\n",
    "autoencoder1.save_weights('ConvAutoEncoderNET1_weights_l1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1205: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From C:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1290: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "(608, 131072)\n"
     ]
    }
   ],
   "source": [
    "encoder1 = Sequential()\n",
    "encoder1.add(Conv2D(32, (3, 3), activation='tanh', padding='same', input_shape=(img_width,img_height,3), name='conv_tanh_1'))\n",
    "encoder1.add(Conv2D(32, (3, 3), activation='tanh', padding='same', name='conv_tanh_2'))\n",
    "encoder1.add(MaxPooling2D((2, 2), padding='same', name='maxpool_1'))\n",
    "encoder1.add(Dropout((0.25), name='dropout_1'))\n",
    "encoder1.add(Flatten(input_shape=(64, 64, 32)))\n",
    "encoder1.load_weights('ConvAutoEncoderNET1_weights_l1', by_name=True)\n",
    "encoder1.compile(optimizer='adadelta', loss='mse')\n",
    "encoder1_out = (encoder1.predict_generator(tuple_generator(pretrain_generator),steps=train_samples/(batch_size*10)))\n",
    "print (encoder1_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 131072)            0         \n",
      "_________________________________________________________________\n",
      "dense_tanh_1 (Dense)         (None, 512)               67109376  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 131072)            67239936  \n",
      "=================================================================\n",
      "Total params: 134,611,968\n",
      "Trainable params: 134,611,968\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "608/608 [==============================] - 7s - loss: 0.0106     \n",
      "Epoch 2/10\n",
      "608/608 [==============================] - 3s - loss: 0.0105     \n",
      "Epoch 3/10\n",
      "608/608 [==============================] - 3s - loss: 0.0105     \n",
      "Epoch 4/10\n",
      "608/608 [==============================] - 3s - loss: 0.0105     \n",
      "Epoch 5/10\n",
      "608/608 [==============================] - 3s - loss: 0.0105     \n",
      "Epoch 6/10\n",
      "608/608 [==============================] - 3s - loss: 0.0105     \n",
      "Epoch 7/10\n",
      "608/608 [==============================] - 3s - loss: 0.0105     \n",
      "Epoch 8/10\n",
      "608/608 [==============================] - 3s - loss: 0.0105     \n",
      "Epoch 9/10\n",
      "608/608 [==============================] - 3s - loss: 0.0105     \n",
      "Epoch 10/10\n",
      "608/608 [==============================] - 3s - loss: 0.0105     \n"
     ]
    }
   ],
   "source": [
    "# second layer's autoencoder\n",
    "input_ = Input(shape=(131072,))\n",
    "x = Dense(units=512, activation='tanh', name='dense_tanh_1')(input_)\n",
    "encoded = Dropout((0.5), name='dropout_2')(x)\n",
    "x = Dense(units=512, activation='tanh')(encoded)\n",
    "decoded = Dense(units=131072)(x)\n",
    "\n",
    "autoencoder2 = Model(input_, decoded)\n",
    "autoencoder2.compile(optimizer='adadelta', loss='mse')\n",
    "print (autoencoder2.summary())\n",
    "autoencoder2.fit(\n",
    "        encoder1_out, encoder1_out,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs\n",
    "        )\n",
    "plot_model(autoencoder2, to_file='ConvAutoEncoder2_l2.png', show_shapes=True, show_layer_names=True, rankdir='LR')\n",
    "autoencoder2.save('ConvAutoEncoderNET1_l2')\n",
    "autoencoder2.save_weights('ConvAutoEncoderNET1_weights_l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "K.clear_session()\n",
    "sess = tf.Session()\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6026 images belonging to 101 classes.\n",
      "Found 2651 images belonging to 101 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Input, Dense, Flatten, Reshape, Dropout, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import SGD\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "def tuple_generator(generator):\n",
    "    for batch in generator:\n",
    "        yield (batch, batch)\n",
    "\n",
    "np.random.seed(777)\n",
    "#if (len(sys.argv)<3):\n",
    "#    print(\"Input arguments:\")\n",
    "#    print(\"1. Train images path\")\n",
    "#    print(\"2. Test images path\")\n",
    "#    exit()\n",
    "PreTrainImagesPath=\"D:/SomeData/Caltech101/TrainImages\"\n",
    "PreTestImagesPath=\"D:/SomeData/Caltech101/TestImages\"\n",
    "TrainImagesPath=\"D:/SomeData/Caltech101/TrainImages\"\n",
    "TestImagesPath=\"D:/SomeData/Caltech101/TestImages\"\n",
    "img_width, img_height = 128, 128\n",
    "epochs = 35\n",
    "batch_size_1 = 16\n",
    "batch_size = 32\n",
    "batch_size_3 = 64\n",
    "dropout_rate = 0.3\n",
    "latent_dim = 101\n",
    "\n",
    "datagen=ImageDataGenerator(rescale=1./255)\n",
    "train_generator = datagen.flow_from_directory(\n",
    "        PreTrainImagesPath,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None)\n",
    "train_samples = train_generator.n\n",
    "\n",
    "test_generator = datagen.flow_from_directory(\n",
    "        PreTestImagesPath,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None)\n",
    "nb_validation_samples = test_generator.n\n",
    "\n",
    "sgd_4 = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "sgd_5 = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "sgd_6 = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 35\n",
    "batch = 32\n",
    "dropout_rate = 0.3\n",
    "latent_dim = 101\n",
    "train_count=6026\n",
    "test_count=2651"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1205: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From C:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2755: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From C:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1290: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='tanh', padding='same', input_shape=(128,128,3), name='conv_tanh_1'))\n",
    "model.add(Conv2D(32, (3, 3), activation='tanh', padding='same', name='conv_tanh_2'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), padding='same', name='maxpool_1'))\n",
    "model.add(Dropout(0.25, name='dropout_1'))\n",
    "model.add(Flatten(name='flat_1'))\n",
    "model.add(Dense(512, activation='tanh', name='dense_tanh_1'))\n",
    "model.add(Dropout(0.5, name='dropout_2'))\n",
    "model.add(Dense(101, activation='softmax', name='dense_softmax_1'))\n",
    "#model.load_weights('ConvAutoEncoderNET1_weights_l1', by_name=True)\n",
    "#model.load_weights('ConvAutoEncoderNET1_weights_l2', by_name=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Output of generator should be a tuple `(x, y, sample_weight)` or `(x, y)`. Found: [[[[ 0.47058827  0.63529414  0.7843138 ]\n   [ 0.45882356  0.62352943  0.77254909]\n   [ 0.46274513  0.627451    0.77647066]\n   ..., \n   [ 0.83529419  0.86666673  0.94117653]\n   [ 0.86666673  0.89411771  0.96470594]\n   [ 0.87843144  0.89803928  0.97254908]]\n\n  [[ 0.45490199  0.61960787  0.76862752]\n   [ 0.44313729  0.60784316  0.75686282]\n   [ 0.44705886  0.61176473  0.76078439]\n   ..., \n   [ 0.82352948  0.85490203  0.92941183]\n   [ 0.85098046  0.87843144  0.95294124]\n   [ 0.86274517  0.88235301  0.95686281]]\n\n  [[ 0.45490199  0.61960787  0.76862752]\n   [ 0.43921572  0.60392159  0.75294125]\n   [ 0.43921572  0.60392159  0.75294125]\n   ..., \n   [ 0.80392164  0.83137262  0.90588242]\n   [ 0.82745105  0.85490203  0.92941183]\n   [ 0.83921576  0.8588236   0.9333334 ]]\n\n  ..., \n  [[ 0.36078432  0.33333334  0.36470589]\n   [ 0.32549021  0.29803923  0.32941177]\n   [ 0.32156864  0.3019608   0.32941177]\n   ..., \n   [ 0.27843139  0.27450982  0.30588236]\n   [ 0.28627452  0.28627452  0.31764707]\n   [ 0.28235295  0.28235295  0.3137255 ]]\n\n  [[ 0.2392157   0.21568629  0.25490198]\n   [ 0.21568629  0.19215688  0.23137257]\n   [ 0.2392157   0.21568629  0.25490198]\n   ..., \n   [ 0.25490198  0.25098041  0.28235295]\n   [ 0.25882354  0.25882354  0.29019609]\n   [ 0.25490198  0.25490198  0.28627452]]\n\n  [[ 0.38431376  0.36078432  0.40000004]\n   [ 0.34117648  0.31764707  0.35686275]\n   [ 0.32156864  0.30980393  0.35294119]\n   ..., \n   [ 0.26666668  0.26274511  0.29411766]\n   [ 0.26666668  0.26666668  0.29803923]\n   [ 0.25882354  0.25882354  0.29019609]]]\n\n\n [[[ 1.          0.98039222  1.        ]\n   [ 1.          0.98431379  1.        ]\n   [ 0.99215692  0.99215692  0.99215692]\n   ..., \n   [ 0.98823535  0.99607849  0.99215692]\n   [ 0.98823535  0.99607849  0.99215692]\n   [ 0.98823535  0.99607849  0.99215692]]\n\n  [[ 1.          0.98039222  1.        ]\n   [ 1.          0.98431379  1.        ]\n   [ 0.99215692  0.99215692  0.99215692]\n   ..., \n   [ 0.98823535  0.99607849  0.99215692]\n   [ 0.98823535  0.99607849  0.99215692]\n   [ 0.98823535  0.99607849  0.99215692]]\n\n  [[ 1.          0.98039222  1.        ]\n   [ 1.          0.98431379  1.        ]\n   [ 0.99215692  0.99215692  0.99215692]\n   ..., \n   [ 0.98823535  0.99607849  0.99215692]\n   [ 0.98823535  0.99607849  0.99215692]\n   [ 0.98823535  0.99607849  0.99215692]]\n\n  ..., \n  [[ 0.98823535  0.99607849  0.99215692]\n   [ 0.98823535  0.99607849  0.99215692]\n   [ 0.98823535  0.99607849  0.99215692]\n   ..., \n   [ 0.97647065  1.          0.99215692]\n   [ 0.96862751  1.          0.99215692]\n   [ 0.96862751  1.          0.99215692]]\n\n  [[ 0.98823535  0.99607849  0.99215692]\n   [ 0.98823535  0.99607849  0.99215692]\n   [ 0.98823535  0.99607849  0.99215692]\n   ..., \n   [ 0.97647065  1.          0.99215692]\n   [ 0.96862751  1.          0.99215692]\n   [ 0.96862751  1.          0.99215692]]\n\n  [[ 0.98823535  0.99607849  0.99215692]\n   [ 0.98823535  0.99607849  0.99215692]\n   [ 0.98823535  0.99607849  0.99215692]\n   ..., \n   [ 0.97647065  1.          0.99215692]\n   [ 0.96862751  1.          0.99215692]\n   [ 0.96470594  1.          0.99215692]]]\n\n\n [[[ 0.65490198  0.30980393  0.01960784]\n   [ 0.63921571  0.29411766  0.00392157]\n   [ 0.63921571  0.28235295  0.00784314]\n   ..., \n   [ 0.59607846  0.24705884  0.01176471]\n   [ 0.58039218  0.24705884  0.        ]\n   [ 0.56862748  0.24313727  0.        ]]\n\n  [[ 0.63921571  0.29411766  0.01176471]\n   [ 0.63137257  0.28627452  0.00392157]\n   [ 0.627451    0.26666668  0.00392157]\n   ..., \n   [ 0.59607846  0.24705884  0.01176471]\n   [ 0.59215689  0.25882354  0.00784314]\n   [ 0.59607846  0.27058825  0.01176471]]\n\n  [[ 0.627451    0.27843139  0.01176471]\n   [ 0.62352943  0.27450982  0.00784314]\n   [ 0.61960787  0.27058825  0.01176471]\n   ..., \n   [ 0.57254905  0.22352943  0.        ]\n   [ 0.57647061  0.2392157   0.        ]\n   [ 0.59215689  0.26274511  0.01176471]]\n\n  ..., \n  [[ 0.05490196  0.05490196  0.05490196]\n   [ 0.05490196  0.05490196  0.05490196]\n   [ 0.05490196  0.05490196  0.05490196]\n   ..., \n   [ 0.18823531  0.10980393  0.11764707]\n   [ 0.18823531  0.10980393  0.11764707]\n   [ 0.18823531  0.10196079  0.1137255 ]]\n\n  [[ 0.01176471  0.01176471  0.01176471]\n   [ 0.01176471  0.01176471  0.01176471]\n   [ 0.01176471  0.01176471  0.01176471]\n   ..., \n   [ 0.10588236  0.02745098  0.03529412]\n   [ 0.10588236  0.02745098  0.03529412]\n   [ 0.10980393  0.02352941  0.03529412]]\n\n  [[ 0.00392157  0.00392157  0.00392157]\n   [ 0.00392157  0.00392157  0.00392157]\n   [ 0.00392157  0.00392157  0.00392157]\n   ..., \n   [ 0.07843138  0.          0.00784314]\n   [ 0.08235294  0.          0.00784314]\n   [ 0.08235294  0.          0.00784314]]]\n\n\n ..., \n [[[ 1.          0.99215692  0.96862751]\n   [ 1.          0.99215692  0.97647065]\n   [ 1.          0.99215692  0.98039222]\n   ..., \n   [ 0.41960788  0.3921569   0.36078432]\n   [ 0.24705884  0.21960786  0.19607845]\n   [ 0.13725491  0.11764707  0.09411766]]\n\n  [[ 1.          0.99215692  0.97647065]\n   [ 1.          0.99607849  0.97647065]\n   [ 1.          0.99215692  0.98039222]\n   ..., \n   [ 0.75686282  0.72941178  0.69803923]\n   [ 0.79215693  0.76470596  0.74117649]\n   [ 0.59607846  0.57647061  0.56078434]]\n\n  [[ 1.          0.99215692  0.98039222]\n   [ 1.          0.99215692  0.98039222]\n   [ 1.          0.99607849  0.98039222]\n   ..., \n   [ 0.56470591  0.53725493  0.51372552]\n   [ 0.49019611  0.47058827  0.44705886]\n   [ 0.34509805  0.32549021  0.30980393]]\n\n  ..., \n  [[ 0.28627452  0.18431373  0.07843138]\n   [ 0.27843139  0.17647059  0.07058824]\n   [ 0.29411766  0.19215688  0.08627451]\n   ..., \n   [ 0.43529415  0.40000004  0.27058825]\n   [ 0.54901963  0.49803925  0.37254903]\n   [ 0.48235297  0.43137258  0.30588236]]\n\n  [[ 0.55686277  0.45098042  0.34509805]\n   [ 0.52549022  0.41960788  0.3137255 ]\n   [ 0.50980395  0.4039216   0.29803923]\n   ..., \n   [ 0.90980399  0.88627458  0.75294125]\n   [ 0.45098042  0.41568631  0.28627452]\n   [ 0.49411768  0.45098042  0.32549021]]\n\n  [[ 0.50588238  0.40000004  0.29411766]\n   [ 0.47058827  0.36470589  0.25882354]\n   [ 0.45882356  0.35294119  0.24705884]\n   ..., \n   [ 0.60392159  0.58823532  0.45098042]\n   [ 0.50980395  0.48627454  0.35294119]\n   [ 0.74117649  0.70588237  0.57647061]]]\n\n\n [[[ 0.          0.01960784  0.        ]\n   [ 0.03137255  0.05490196  0.00784314]\n   [ 0.01568628  0.03921569  0.        ]\n   ..., \n   [ 0.01960784  0.09019608  0.09019608]\n   [ 0.02352941  0.09411766  0.09411766]\n   [ 0.02745098  0.09803922  0.09803922]]\n\n  [[ 0.          0.01176471  0.        ]\n   [ 0.01960784  0.04313726  0.        ]\n   [ 0.          0.01960784  0.        ]\n   ..., \n   [ 0.01568628  0.07843138  0.07843138]\n   [ 0.          0.0627451   0.0627451 ]\n   [ 0.          0.0509804   0.0509804 ]]\n\n  [[ 0.17647059  0.20000002  0.15294118]\n   [ 0.12941177  0.15294118  0.10588236]\n   [ 0.03921569  0.0627451   0.01568628]\n   ..., \n   [ 0.02745098  0.09019608  0.09019608]\n   [ 0.00784314  0.07058824  0.07058824]\n   [ 0.          0.0627451   0.0627451 ]]\n\n  ..., \n  [[ 0.0509804   0.0509804   0.0509804 ]\n   [ 0.01568628  0.01568628  0.01568628]\n   [ 0.03529412  0.03529412  0.03529412]\n   ..., \n   [ 0.08627451  0.06666667  0.08235294]\n   [ 0.05882353  0.03529412  0.0509804 ]\n   [ 0.04313726  0.00784314  0.02745098]]\n\n  [[ 0.02745098  0.02745098  0.02745098]\n   [ 0.          0.          0.        ]\n   [ 0.          0.          0.        ]\n   ..., \n   [ 0.06666667  0.04705883  0.0627451 ]\n   [ 0.04313726  0.00784314  0.02745098]\n   [ 0.04705883  0.01176471  0.03137255]]\n\n  [[ 0.00784314  0.00784314  0.00784314]\n   [ 0.02352941  0.02352941  0.02352941]\n   [ 0.00392157  0.00392157  0.00392157]\n   ..., \n   [ 0.01176471  0.          0.00784314]\n   [ 0.02352941  0.          0.00784314]\n   [ 0.03921569  0.          0.01960784]]]\n\n\n [[[ 0.08235294  0.22352943  0.41960788]\n   [ 0.09803922  0.2392157   0.43529415]\n   [ 0.10980393  0.25098041  0.44705886]\n   ..., \n   [ 0.08235294  0.20392159  0.38823533]\n   [ 0.09019608  0.21176472  0.39607847]\n   [ 0.09019608  0.21176472  0.39607847]]\n\n  [[ 0.01568628  0.15686275  0.35294119]\n   [ 0.02352941  0.16470589  0.36078432]\n   [ 0.03529412  0.17647059  0.37254903]\n   ..., \n   [ 0.0627451   0.18823531  0.37254903]\n   [ 0.07450981  0.20000002  0.38431376]\n   [ 0.09019608  0.21568629  0.40000004]]\n\n  [[ 0.09019608  0.23137257  0.42745101]\n   [ 0.08235294  0.22352943  0.41960788]\n   [ 0.07843138  0.21960786  0.41568631]\n   ..., \n   [ 0.05882353  0.18431373  0.36862746]\n   [ 0.07843138  0.20392159  0.38823533]\n   [ 0.09803922  0.22352943  0.40784317]]\n\n  ..., \n  [[ 0.25490198  0.46274513  0.67450982]\n   [ 0.25098041  0.45882356  0.67058825]\n   [ 0.25098041  0.45882356  0.67058825]\n   ..., \n   [ 0.22352943  0.42352945  0.60000002]\n   [ 0.22352943  0.42352945  0.60000002]\n   [ 0.227451    0.42745101  0.60392159]]\n\n  [[ 0.25098041  0.45882356  0.67058825]\n   [ 0.25098041  0.45882356  0.67058825]\n   [ 0.25490198  0.46274513  0.67450982]\n   ..., \n   [ 0.22352943  0.42352945  0.60000002]\n   [ 0.22352943  0.42352945  0.60000002]\n   [ 0.22352943  0.42352945  0.60000002]]\n\n  [[ 0.24705884  0.45490199  0.66666669]\n   [ 0.25098041  0.45882356  0.67058825]\n   [ 0.25882354  0.4666667   0.67843139]\n   ..., \n   [ 0.22352943  0.42352945  0.60000002]\n   [ 0.22352943  0.42352945  0.60000002]\n   [ 0.22352943  0.42352945  0.60000002]]]]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-2988af48da4c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_count\u001b[0m\u001b[1;33m//\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         callbacks=[early_stopping])\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mt1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mloss_and_metrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_generator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_count\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 87\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, initial_epoch)\u001b[0m\n\u001b[0;32m   1119\u001b[0m                                         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1120\u001b[0m                                         \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 87\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2025\u001b[0m                                          \u001b[1;34m'a tuple `(x, y, sample_weight)` '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2026\u001b[0m                                          \u001b[1;34m'or `(x, y)`. Found: '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2027\u001b[1;33m                                          str(generator_output))\n\u001b[0m\u001b[0;32m   2028\u001b[0m                     \u001b[1;31m# build batch logs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2029\u001b[0m                     \u001b[0mbatch_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Output of generator should be a tuple `(x, y, sample_weight)` or `(x, y)`. Found: [[[[ 0.47058827  0.63529414  0.7843138 ]\n   [ 0.45882356  0.62352943  0.77254909]\n   [ 0.46274513  0.627451    0.77647066]\n   ..., \n   [ 0.83529419  0.86666673  0.94117653]\n   [ 0.86666673  0.89411771  0.96470594]\n   [ 0.87843144  0.89803928  0.97254908]]\n\n  [[ 0.45490199  0.61960787  0.76862752]\n   [ 0.44313729  0.60784316  0.75686282]\n   [ 0.44705886  0.61176473  0.76078439]\n   ..., \n   [ 0.82352948  0.85490203  0.92941183]\n   [ 0.85098046  0.87843144  0.95294124]\n   [ 0.86274517  0.88235301  0.95686281]]\n\n  [[ 0.45490199  0.61960787  0.76862752]\n   [ 0.43921572  0.60392159  0.75294125]\n   [ 0.43921572  0.60392159  0.75294125]\n   ..., \n   [ 0.80392164  0.83137262  0.90588242]\n   [ 0.82745105  0.85490203  0.92941183]\n   [ 0.83921576  0.8588236   0.9333334 ]]\n\n  ..., \n  [[ 0.36078432  0.33333334  0.36470589]\n   [ 0.32549021  0.29803923  0.32941177]\n   [ 0.32156864  0.3019608   0.32941177]\n   ..., \n   [ 0.27843139  0.27450982  0.30588236]\n   [ 0.28627452  0.28627452  0.31764707]\n   [ 0.28235295  0.28235295  0.3137255 ]]\n\n  [[ 0.2392157   0.21568629  0.25490198]\n   [ 0.21568629  0.19215688  0.23137257]\n   [ 0.2392157   0.21568629  0.25490198]\n   ..., \n   [ 0.25490198  0.25098041  0.28235295]\n   [ 0.25882354  0.25882354  0.29019609]\n   [ 0.25490198  0.25490198  0.28627452]]\n\n  [[ 0.38431376  0.36078432  0.40000004]\n   [ 0.34117648  0.31764707  0.35686275]\n   [ 0.32156864  0.30980393  0.35294119]\n   ..., \n   [ 0.26666668  0.26274511  0.29411766]\n   [ 0.26666668  0.26666668  0.29803923]\n   [ 0.25882354  0.25882354  0.29019609]]]\n\n\n [[[ 1.          0.98039222  1.        ]\n   [ 1.          0.98431379  1.        ]\n   [ 0.99215692  0.99215692  0.99215692]\n   ..., \n   [ 0.98823535  0.99607849  0.99215692]\n   [ 0.98823535  0.99607849  0.99215692]\n   [ 0.98823535  0.99607849  0.99215692]]\n\n  [[ 1.          0.98039222  1.        ]\n   [ 1.          0.98431379  1.        ]\n   [ 0.99215692  0.99215692  0.99215692]\n   ..., \n   [ 0.98823535  0.99607849  0.99215692]\n   [ 0.98823535  0.99607849  0.99215692]\n   [ 0.98823535  0.99607849  0.99215692]]\n\n  [[ 1.          0.98039222  1.        ]\n   [ 1.          0.98431379  1.        ]\n   [ 0.99215692  0.99215692  0.99215692]\n   ..., \n   [ 0.98823535  0.99607849  0.99215692]\n   [ 0.98823535  0.99607849  0.99215692]\n   [ 0.98823535  0.99607849  0.99215692]]\n\n  ..., \n  [[ 0.98823535  0.99607849  0.99215692]\n   [ 0.98823535  0.99607849  0.99215692]\n   [ 0.98823535  0.99607849  0.99215692]\n   ..., \n   [ 0.97647065  1.          0.99215692]\n   [ 0.96862751  1.          0.99215692]\n   [ 0.96862751  1.          0.99215692]]\n\n  [[ 0.98823535  0.99607849  0.99215692]\n   [ 0.98823535  0.99607849  0.99215692]\n   [ 0.98823535  0.99607849  0.99215692]\n   ..., \n   [ 0.97647065  1.          0.99215692]\n   [ 0.96862751  1.          0.99215692]\n   [ 0.96862751  1.          0.99215692]]\n\n  [[ 0.98823535  0.99607849  0.99215692]\n   [ 0.98823535  0.99607849  0.99215692]\n   [ 0.98823535  0.99607849  0.99215692]\n   ..., \n   [ 0.97647065  1.          0.99215692]\n   [ 0.96862751  1.          0.99215692]\n   [ 0.96470594  1.          0.99215692]]]\n\n\n [[[ 0.65490198  0.30980393  0.01960784]\n   [ 0.63921571  0.29411766  0.00392157]\n   [ 0.63921571  0.28235295  0.00784314]\n   ..., \n   [ 0.59607846  0.24705884  0.01176471]\n   [ 0.58039218  0.24705884  0.        ]\n   [ 0.56862748  0.24313727  0.        ]]\n\n  [[ 0.63921571  0.29411766  0.01176471]\n   [ 0.63137257  0.28627452  0.00392157]\n   [ 0.627451    0.26666668  0.00392157]\n   ..., \n   [ 0.59607846  0.24705884  0.01176471]\n   [ 0.59215689  0.25882354  0.00784314]\n   [ 0.59607846  0.27058825  0.01176471]]\n\n  [[ 0.627451    0.27843139  0.01176471]\n   [ 0.62352943  0.27450982  0.00784314]\n   [ 0.61960787  0.27058825  0.01176471]\n   ..., \n   [ 0.57254905  0.22352943  0.        ]\n   [ 0.57647061  0.2392157   0.        ]\n   [ 0.59215689  0.26274511  0.01176471]]\n\n  ..., \n  [[ 0.05490196  0.05490196  0.05490196]\n   [ 0.05490196  0.05490196  0.05490196]\n   [ 0.05490196  0.05490196  0.05490196]\n   ..., \n   [ 0.18823531  0.10980393  0.11764707]\n   [ 0.18823531  0.10980393  0.11764707]\n   [ 0.18823531  0.10196079  0.1137255 ]]\n\n  [[ 0.01176471  0.01176471  0.01176471]\n   [ 0.01176471  0.01176471  0.01176471]\n   [ 0.01176471  0.01176471  0.01176471]\n   ..., \n   [ 0.10588236  0.02745098  0.03529412]\n   [ 0.10588236  0.02745098  0.03529412]\n   [ 0.10980393  0.02352941  0.03529412]]\n\n  [[ 0.00392157  0.00392157  0.00392157]\n   [ 0.00392157  0.00392157  0.00392157]\n   [ 0.00392157  0.00392157  0.00392157]\n   ..., \n   [ 0.07843138  0.          0.00784314]\n   [ 0.08235294  0.          0.00784314]\n   [ 0.08235294  0.          0.00784314]]]\n\n\n ..., \n [[[ 1.          0.99215692  0.96862751]\n   [ 1.          0.99215692  0.97647065]\n   [ 1.          0.99215692  0.98039222]\n   ..., \n   [ 0.41960788  0.3921569   0.36078432]\n   [ 0.24705884  0.21960786  0.19607845]\n   [ 0.13725491  0.11764707  0.09411766]]\n\n  [[ 1.          0.99215692  0.97647065]\n   [ 1.          0.99607849  0.97647065]\n   [ 1.          0.99215692  0.98039222]\n   ..., \n   [ 0.75686282  0.72941178  0.69803923]\n   [ 0.79215693  0.76470596  0.74117649]\n   [ 0.59607846  0.57647061  0.56078434]]\n\n  [[ 1.          0.99215692  0.98039222]\n   [ 1.          0.99215692  0.98039222]\n   [ 1.          0.99607849  0.98039222]\n   ..., \n   [ 0.56470591  0.53725493  0.51372552]\n   [ 0.49019611  0.47058827  0.44705886]\n   [ 0.34509805  0.32549021  0.30980393]]\n\n  ..., \n  [[ 0.28627452  0.18431373  0.07843138]\n   [ 0.27843139  0.17647059  0.07058824]\n   [ 0.29411766  0.19215688  0.08627451]\n   ..., \n   [ 0.43529415  0.40000004  0.27058825]\n   [ 0.54901963  0.49803925  0.37254903]\n   [ 0.48235297  0.43137258  0.30588236]]\n\n  [[ 0.55686277  0.45098042  0.34509805]\n   [ 0.52549022  0.41960788  0.3137255 ]\n   [ 0.50980395  0.4039216   0.29803923]\n   ..., \n   [ 0.90980399  0.88627458  0.75294125]\n   [ 0.45098042  0.41568631  0.28627452]\n   [ 0.49411768  0.45098042  0.32549021]]\n\n  [[ 0.50588238  0.40000004  0.29411766]\n   [ 0.47058827  0.36470589  0.25882354]\n   [ 0.45882356  0.35294119  0.24705884]\n   ..., \n   [ 0.60392159  0.58823532  0.45098042]\n   [ 0.50980395  0.48627454  0.35294119]\n   [ 0.74117649  0.70588237  0.57647061]]]\n\n\n [[[ 0.          0.01960784  0.        ]\n   [ 0.03137255  0.05490196  0.00784314]\n   [ 0.01568628  0.03921569  0.        ]\n   ..., \n   [ 0.01960784  0.09019608  0.09019608]\n   [ 0.02352941  0.09411766  0.09411766]\n   [ 0.02745098  0.09803922  0.09803922]]\n\n  [[ 0.          0.01176471  0.        ]\n   [ 0.01960784  0.04313726  0.        ]\n   [ 0.          0.01960784  0.        ]\n   ..., \n   [ 0.01568628  0.07843138  0.07843138]\n   [ 0.          0.0627451   0.0627451 ]\n   [ 0.          0.0509804   0.0509804 ]]\n\n  [[ 0.17647059  0.20000002  0.15294118]\n   [ 0.12941177  0.15294118  0.10588236]\n   [ 0.03921569  0.0627451   0.01568628]\n   ..., \n   [ 0.02745098  0.09019608  0.09019608]\n   [ 0.00784314  0.07058824  0.07058824]\n   [ 0.          0.0627451   0.0627451 ]]\n\n  ..., \n  [[ 0.0509804   0.0509804   0.0509804 ]\n   [ 0.01568628  0.01568628  0.01568628]\n   [ 0.03529412  0.03529412  0.03529412]\n   ..., \n   [ 0.08627451  0.06666667  0.08235294]\n   [ 0.05882353  0.03529412  0.0509804 ]\n   [ 0.04313726  0.00784314  0.02745098]]\n\n  [[ 0.02745098  0.02745098  0.02745098]\n   [ 0.          0.          0.        ]\n   [ 0.          0.          0.        ]\n   ..., \n   [ 0.06666667  0.04705883  0.0627451 ]\n   [ 0.04313726  0.00784314  0.02745098]\n   [ 0.04705883  0.01176471  0.03137255]]\n\n  [[ 0.00784314  0.00784314  0.00784314]\n   [ 0.02352941  0.02352941  0.02352941]\n   [ 0.00392157  0.00392157  0.00392157]\n   ..., \n   [ 0.01176471  0.          0.00784314]\n   [ 0.02352941  0.          0.00784314]\n   [ 0.03921569  0.          0.01960784]]]\n\n\n [[[ 0.08235294  0.22352943  0.41960788]\n   [ 0.09803922  0.2392157   0.43529415]\n   [ 0.10980393  0.25098041  0.44705886]\n   ..., \n   [ 0.08235294  0.20392159  0.38823533]\n   [ 0.09019608  0.21176472  0.39607847]\n   [ 0.09019608  0.21176472  0.39607847]]\n\n  [[ 0.01568628  0.15686275  0.35294119]\n   [ 0.02352941  0.16470589  0.36078432]\n   [ 0.03529412  0.17647059  0.37254903]\n   ..., \n   [ 0.0627451   0.18823531  0.37254903]\n   [ 0.07450981  0.20000002  0.38431376]\n   [ 0.09019608  0.21568629  0.40000004]]\n\n  [[ 0.09019608  0.23137257  0.42745101]\n   [ 0.08235294  0.22352943  0.41960788]\n   [ 0.07843138  0.21960786  0.41568631]\n   ..., \n   [ 0.05882353  0.18431373  0.36862746]\n   [ 0.07843138  0.20392159  0.38823533]\n   [ 0.09803922  0.22352943  0.40784317]]\n\n  ..., \n  [[ 0.25490198  0.46274513  0.67450982]\n   [ 0.25098041  0.45882356  0.67058825]\n   [ 0.25098041  0.45882356  0.67058825]\n   ..., \n   [ 0.22352943  0.42352945  0.60000002]\n   [ 0.22352943  0.42352945  0.60000002]\n   [ 0.227451    0.42745101  0.60392159]]\n\n  [[ 0.25098041  0.45882356  0.67058825]\n   [ 0.25098041  0.45882356  0.67058825]\n   [ 0.25490198  0.46274513  0.67450982]\n   ..., \n   [ 0.22352943  0.42352945  0.60000002]\n   [ 0.22352943  0.42352945  0.60000002]\n   [ 0.22352943  0.42352945  0.60000002]]\n\n  [[ 0.24705884  0.45490199  0.66666669]\n   [ 0.25098041  0.45882356  0.67058825]\n   [ 0.25882354  0.4666667   0.67843139]\n   ..., \n   [ 0.22352943  0.42352945  0.60000002]\n   [ 0.22352943  0.42352945  0.60000002]\n   [ 0.22352943  0.42352945  0.60000002]]]]"
     ]
    }
   ],
   "source": [
    "early_stopping=EarlyStopping(monitor='acc', patience=3, verbose=0, mode='auto')\n",
    "t0=time.time()\n",
    "model.fit_generator(train_generator,\n",
    "        steps_per_epoch=train_count//batch,\n",
    "        epochs=epochs,\n",
    "        callbacks=[early_stopping])\n",
    "t1=time.time()\n",
    "loss_and_metrics = model.evaluate_generator(test_generator, steps=test_count/batch)\n",
    "print('Accuracy =',loss_and_metrics[1])\n",
    "print('Time =',(t1-t0))\n",
    "#%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "K.clear_session()\n",
    "sess = tf.Session()\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Output of generator should be a tuple `(x, y, sample_weight)` or `(x, y)`. Found: [[[[ 0.00392157  0.          0.        ]\n   [ 0.05490196  0.04705883  0.0509804 ]\n   [ 0.05882353  0.0509804   0.05490196]\n   ..., \n   [ 0.03921569  0.03137255  0.04313726]\n   [ 0.04313726  0.03529412  0.05490196]\n   [ 0.01568628  0.00784314  0.02745098]]\n\n  [[ 0.07843138  0.07058824  0.07450981]\n   [ 0.0509804   0.04313726  0.04705883]\n   [ 0.00392157  0.          0.        ]\n   ..., \n   [ 0.03137255  0.02352941  0.03529412]\n   [ 0.04313726  0.03529412  0.04705883]\n   [ 0.01568628  0.00784314  0.01960784]]\n\n  [[ 0.03529412  0.02745098  0.03137255]\n   [ 0.00392157  0.          0.        ]\n   [ 0.06666667  0.05882353  0.0627451 ]\n   ..., \n   [ 0.02745098  0.01960784  0.03137255]\n   [ 0.04705883  0.02745098  0.04313726]\n   [ 0.01960784  0.          0.01568628]]\n\n  ..., \n  [[ 0.19215688  0.21176472  0.13333334]\n   [ 0.17647059  0.19607845  0.11764707]\n   [ 0.15294118  0.17254902  0.09411766]\n   ..., \n   [ 0.13725491  0.14509805  0.09019608]\n   [ 0.10980393  0.11764707  0.0627451 ]\n   [ 0.02352941  0.03137255  0.        ]]\n\n  [[ 0.19215688  0.21176472  0.1254902 ]\n   [ 0.18039216  0.20000002  0.1137255 ]\n   [ 0.16078432  0.18039216  0.09411766]\n   ..., \n   [ 0.24705884  0.25882354  0.19215688]\n   [ 0.18823531  0.19607845  0.14117648]\n   [ 0.01960784  0.02745098  0.        ]]\n\n  [[ 0.14117648  0.16078432  0.07450981]\n   [ 0.15294118  0.17254902  0.08627451]\n   [ 0.16470589  0.18431373  0.09803922]\n   ..., \n   [ 0.32941177  0.34117648  0.26666668]\n   [ 0.24313727  0.25490198  0.18823531]\n   [ 0.          0.01176471  0.        ]]]\n\n\n [[[ 0.84705889  0.84705889  0.84705889]\n   [ 0.95686281  0.95686281  0.95686281]\n   [ 0.76862752  0.76862752  0.76862752]\n   ..., \n   [ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]]\n\n  [[ 0.56078434  0.56078434  0.56078434]\n   [ 0.82352948  0.82352948  0.82352948]\n   [ 0.80000007  0.80000007  0.80000007]\n   ..., \n   [ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]]\n\n  [[ 1.          1.          1.        ]\n   [ 0.78039223  0.78039223  0.78039223]\n   [ 0.78823537  0.78823537  0.78823537]\n   ..., \n   [ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]]\n\n  ..., \n  [[ 0.6901961   0.6901961   0.6901961 ]\n   [ 0.63137257  0.63137257  0.63137257]\n   [ 0.66274512  0.66274512  0.66274512]\n   ..., \n   [ 0.96078438  0.96078438  0.96078438]\n   [ 0.97254908  0.97254908  0.97254908]\n   [ 1.          1.          1.        ]]\n\n  [[ 0.99215692  0.99215692  0.99215692]\n   [ 0.91764712  0.91764712  0.91764712]\n   [ 1.          1.          1.        ]\n   ..., \n   [ 0.98823535  0.98823535  0.98823535]\n   [ 0.98823535  0.98823535  0.98823535]\n   [ 0.98823535  0.98823535  0.98823535]]\n\n  [[ 0.76470596  0.76470596  0.76470596]\n   [ 0.80784321  0.80784321  0.80784321]\n   [ 0.78823537  0.78823537  0.78823537]\n   ..., \n   [ 0.99607849  0.99607849  0.99607849]\n   [ 1.          1.          1.        ]\n   [ 0.99215692  0.99215692  0.99215692]]]\n\n\n [[[ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]\n   ..., \n   [ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]]\n\n  [[ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]\n   ..., \n   [ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]]\n\n  [[ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]\n   ..., \n   [ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]]\n\n  ..., \n  [[ 0.90980399  0.90980399  0.90980399]\n   [ 0.96078438  0.96078438  0.96078438]\n   [ 1.          1.          1.        ]\n   ..., \n   [ 0.90588242  0.90588242  0.90588242]\n   [ 0.93725497  0.93725497  0.93725497]\n   [ 0.99607849  0.99607849  0.99607849]]\n\n  [[ 1.          1.          1.        ]\n   [ 0.98039222  0.98039222  0.98039222]\n   [ 0.97647065  0.97647065  0.97647065]\n   ..., \n   [ 1.          1.          1.        ]\n   [ 0.97647065  0.97647065  0.97647065]\n   [ 1.          1.          1.        ]]\n\n  [[ 0.96078438  0.96078438  0.96078438]\n   [ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]\n   ..., \n   [ 0.95294124  0.95294124  0.95294124]\n   [ 1.          1.          1.        ]\n   [ 0.98823535  0.98823535  0.98823535]]]\n\n\n ..., \n [[[ 0.9333334   0.84313732  0.        ]\n   [ 0.8705883   0.7960785   0.01960784]\n   [ 0.96078438  0.91764712  0.25882354]\n   ..., \n   [ 1.          0.96862751  0.54901963]\n   [ 0.99215692  0.88235301  0.25098041]\n   [ 1.          0.99215692  0.17647059]]\n\n  [[ 0.88627458  0.7019608   0.        ]\n   [ 0.76078439  0.59215689  0.        ]\n   [ 0.93725497  0.80000007  0.23529413]\n   ..., \n   [ 0.97647065  0.80000007  0.39607847]\n   [ 0.96862751  0.81176478  0.19215688]\n   [ 0.98039222  0.84313732  0.03529412]]\n\n  [[ 0.9450981   0.67450982  0.03529412]\n   [ 0.7960785   0.5411765   0.        ]\n   [ 1.          0.83921576  0.4039216 ]\n   ..., \n   [ 0.80784321  0.6156863   0.3137255 ]\n   [ 1.          0.81568635  0.30588236]\n   [ 0.93725497  0.77254909  0.06666667]]\n\n  ..., \n  [[ 0.91372555  0.7019608   0.03529412]\n   [ 0.76862752  0.56862748  0.        ]\n   [ 0.72549021  0.5411765   0.        ]\n   ..., \n   [ 0.74509805  0.57647061  0.        ]\n   [ 0.76470596  0.58039218  0.        ]\n   [ 0.81176478  0.62352943  0.        ]]\n\n  [[ 1.          0.79215693  0.18823531]\n   [ 0.91372555  0.69803923  0.09019608]\n   [ 0.80000007  0.60000002  0.        ]\n   ..., \n   [ 0.81568635  0.61176473  0.08235294]\n   [ 0.7960785   0.58039218  0.01568628]\n   [ 0.85490203  0.63137257  0.03529412]]\n\n  [[ 0.95294124  0.71372551  0.15294118]\n   [ 0.96078438  0.73333335  0.16862746]\n   [ 0.91372555  0.7019608   0.12156864]\n   ..., \n   [ 0.90980399  0.68235296  0.25098041]\n   [ 0.86274517  0.62352943  0.16470589]\n   [ 0.92156869  0.67058825  0.18431373]]]\n\n\n [[[ 0.          0.          0.        ]\n   [ 0.          0.          0.        ]\n   [ 0.          0.          0.        ]\n   ..., \n   [ 0.          0.          0.        ]\n   [ 0.          0.          0.        ]\n   [ 0.          0.          0.        ]]\n\n  [[ 0.          0.          0.        ]\n   [ 0.          0.          0.        ]\n   [ 0.          0.          0.        ]\n   ..., \n   [ 0.          0.          0.        ]\n   [ 0.          0.          0.        ]\n   [ 0.          0.          0.        ]]\n\n  [[ 0.          0.          0.        ]\n   [ 0.          0.          0.        ]\n   [ 0.          0.          0.        ]\n   ..., \n   [ 0.          0.          0.        ]\n   [ 0.          0.          0.        ]\n   [ 0.          0.          0.        ]]\n\n  ..., \n  [[ 0.          0.          0.        ]\n   [ 0.          0.          0.        ]\n   [ 0.          0.          0.        ]\n   ..., \n   [ 0.          0.          0.        ]\n   [ 0.          0.          0.        ]\n   [ 0.          0.          0.        ]]\n\n  [[ 0.          0.          0.        ]\n   [ 0.          0.          0.        ]\n   [ 0.          0.          0.        ]\n   ..., \n   [ 0.          0.          0.        ]\n   [ 0.          0.          0.        ]\n   [ 0.          0.          0.        ]]\n\n  [[ 0.          0.          0.        ]\n   [ 0.          0.          0.        ]\n   [ 0.          0.          0.        ]\n   ..., \n   [ 0.          0.          0.        ]\n   [ 0.          0.          0.        ]\n   [ 0.          0.          0.        ]]]\n\n\n [[[ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]\n   ..., \n   [ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]]\n\n  [[ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]\n   ..., \n   [ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]]\n\n  [[ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]\n   ..., \n   [ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]]\n\n  ..., \n  [[ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]\n   ..., \n   [ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]]\n\n  [[ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]\n   ..., \n   [ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]]\n\n  [[ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]\n   ..., \n   [ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]]]]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-e74f27a70509>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_count\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         callbacks=[early_stopping])\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[0mt1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mloss_and_metrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_generator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_count\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 87\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, initial_epoch)\u001b[0m\n\u001b[0;32m   1119\u001b[0m                                         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1120\u001b[0m                                         \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 87\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2025\u001b[0m                                          \u001b[1;34m'a tuple `(x, y, sample_weight)` '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2026\u001b[0m                                          \u001b[1;34m'or `(x, y)`. Found: '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2027\u001b[1;33m                                          str(generator_output))\n\u001b[0m\u001b[0;32m   2028\u001b[0m                     \u001b[1;31m# build batch logs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2029\u001b[0m                     \u001b[0mbatch_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Output of generator should be a tuple `(x, y, sample_weight)` or `(x, y)`. Found: [[[[ 0.00392157  0.          0.        ]\n   [ 0.05490196  0.04705883  0.0509804 ]\n   [ 0.05882353  0.0509804   0.05490196]\n   ..., \n   [ 0.03921569  0.03137255  0.04313726]\n   [ 0.04313726  0.03529412  0.05490196]\n   [ 0.01568628  0.00784314  0.02745098]]\n\n  [[ 0.07843138  0.07058824  0.07450981]\n   [ 0.0509804   0.04313726  0.04705883]\n   [ 0.00392157  0.          0.        ]\n   ..., \n   [ 0.03137255  0.02352941  0.03529412]\n   [ 0.04313726  0.03529412  0.04705883]\n   [ 0.01568628  0.00784314  0.01960784]]\n\n  [[ 0.03529412  0.02745098  0.03137255]\n   [ 0.00392157  0.          0.        ]\n   [ 0.06666667  0.05882353  0.0627451 ]\n   ..., \n   [ 0.02745098  0.01960784  0.03137255]\n   [ 0.04705883  0.02745098  0.04313726]\n   [ 0.01960784  0.          0.01568628]]\n\n  ..., \n  [[ 0.19215688  0.21176472  0.13333334]\n   [ 0.17647059  0.19607845  0.11764707]\n   [ 0.15294118  0.17254902  0.09411766]\n   ..., \n   [ 0.13725491  0.14509805  0.09019608]\n   [ 0.10980393  0.11764707  0.0627451 ]\n   [ 0.02352941  0.03137255  0.        ]]\n\n  [[ 0.19215688  0.21176472  0.1254902 ]\n   [ 0.18039216  0.20000002  0.1137255 ]\n   [ 0.16078432  0.18039216  0.09411766]\n   ..., \n   [ 0.24705884  0.25882354  0.19215688]\n   [ 0.18823531  0.19607845  0.14117648]\n   [ 0.01960784  0.02745098  0.        ]]\n\n  [[ 0.14117648  0.16078432  0.07450981]\n   [ 0.15294118  0.17254902  0.08627451]\n   [ 0.16470589  0.18431373  0.09803922]\n   ..., \n   [ 0.32941177  0.34117648  0.26666668]\n   [ 0.24313727  0.25490198  0.18823531]\n   [ 0.          0.01176471  0.        ]]]\n\n\n [[[ 0.84705889  0.84705889  0.84705889]\n   [ 0.95686281  0.95686281  0.95686281]\n   [ 0.76862752  0.76862752  0.76862752]\n   ..., \n   [ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]]\n\n  [[ 0.56078434  0.56078434  0.56078434]\n   [ 0.82352948  0.82352948  0.82352948]\n   [ 0.80000007  0.80000007  0.80000007]\n   ..., \n   [ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]]\n\n  [[ 1.          1.          1.        ]\n   [ 0.78039223  0.78039223  0.78039223]\n   [ 0.78823537  0.78823537  0.78823537]\n   ..., \n   [ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]]\n\n  ..., \n  [[ 0.6901961   0.6901961   0.6901961 ]\n   [ 0.63137257  0.63137257  0.63137257]\n   [ 0.66274512  0.66274512  0.66274512]\n   ..., \n   [ 0.96078438  0.96078438  0.96078438]\n   [ 0.97254908  0.97254908  0.97254908]\n   [ 1.          1.          1.        ]]\n\n  [[ 0.99215692  0.99215692  0.99215692]\n   [ 0.91764712  0.91764712  0.91764712]\n   [ 1.          1.          1.        ]\n   ..., \n   [ 0.98823535  0.98823535  0.98823535]\n   [ 0.98823535  0.98823535  0.98823535]\n   [ 0.98823535  0.98823535  0.98823535]]\n\n  [[ 0.76470596  0.76470596  0.76470596]\n   [ 0.80784321  0.80784321  0.80784321]\n   [ 0.78823537  0.78823537  0.78823537]\n   ..., \n   [ 0.99607849  0.99607849  0.99607849]\n   [ 1.          1.          1.        ]\n   [ 0.99215692  0.99215692  0.99215692]]]\n\n\n [[[ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]\n   ..., \n   [ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]]\n\n  [[ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]\n   ..., \n   [ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]]\n\n  [[ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]\n   ..., \n   [ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]]\n\n  ..., \n  [[ 0.90980399  0.90980399  0.90980399]\n   [ 0.96078438  0.96078438  0.96078438]\n   [ 1.          1.          1.        ]\n   ..., \n   [ 0.90588242  0.90588242  0.90588242]\n   [ 0.93725497  0.93725497  0.93725497]\n   [ 0.99607849  0.99607849  0.99607849]]\n\n  [[ 1.          1.          1.        ]\n   [ 0.98039222  0.98039222  0.98039222]\n   [ 0.97647065  0.97647065  0.97647065]\n   ..., \n   [ 1.          1.          1.        ]\n   [ 0.97647065  0.97647065  0.97647065]\n   [ 1.          1.          1.        ]]\n\n  [[ 0.96078438  0.96078438  0.96078438]\n   [ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]\n   ..., \n   [ 0.95294124  0.95294124  0.95294124]\n   [ 1.          1.          1.        ]\n   [ 0.98823535  0.98823535  0.98823535]]]\n\n\n ..., \n [[[ 0.9333334   0.84313732  0.        ]\n   [ 0.8705883   0.7960785   0.01960784]\n   [ 0.96078438  0.91764712  0.25882354]\n   ..., \n   [ 1.          0.96862751  0.54901963]\n   [ 0.99215692  0.88235301  0.25098041]\n   [ 1.          0.99215692  0.17647059]]\n\n  [[ 0.88627458  0.7019608   0.        ]\n   [ 0.76078439  0.59215689  0.        ]\n   [ 0.93725497  0.80000007  0.23529413]\n   ..., \n   [ 0.97647065  0.80000007  0.39607847]\n   [ 0.96862751  0.81176478  0.19215688]\n   [ 0.98039222  0.84313732  0.03529412]]\n\n  [[ 0.9450981   0.67450982  0.03529412]\n   [ 0.7960785   0.5411765   0.        ]\n   [ 1.          0.83921576  0.4039216 ]\n   ..., \n   [ 0.80784321  0.6156863   0.3137255 ]\n   [ 1.          0.81568635  0.30588236]\n   [ 0.93725497  0.77254909  0.06666667]]\n\n  ..., \n  [[ 0.91372555  0.7019608   0.03529412]\n   [ 0.76862752  0.56862748  0.        ]\n   [ 0.72549021  0.5411765   0.        ]\n   ..., \n   [ 0.74509805  0.57647061  0.        ]\n   [ 0.76470596  0.58039218  0.        ]\n   [ 0.81176478  0.62352943  0.        ]]\n\n  [[ 1.          0.79215693  0.18823531]\n   [ 0.91372555  0.69803923  0.09019608]\n   [ 0.80000007  0.60000002  0.        ]\n   ..., \n   [ 0.81568635  0.61176473  0.08235294]\n   [ 0.7960785   0.58039218  0.01568628]\n   [ 0.85490203  0.63137257  0.03529412]]\n\n  [[ 0.95294124  0.71372551  0.15294118]\n   [ 0.96078438  0.73333335  0.16862746]\n   [ 0.91372555  0.7019608   0.12156864]\n   ..., \n   [ 0.90980399  0.68235296  0.25098041]\n   [ 0.86274517  0.62352943  0.16470589]\n   [ 0.92156869  0.67058825  0.18431373]]]\n\n\n [[[ 0.          0.          0.        ]\n   [ 0.          0.          0.        ]\n   [ 0.          0.          0.        ]\n   ..., \n   [ 0.          0.          0.        ]\n   [ 0.          0.          0.        ]\n   [ 0.          0.          0.        ]]\n\n  [[ 0.          0.          0.        ]\n   [ 0.          0.          0.        ]\n   [ 0.          0.          0.        ]\n   ..., \n   [ 0.          0.          0.        ]\n   [ 0.          0.          0.        ]\n   [ 0.          0.          0.        ]]\n\n  [[ 0.          0.          0.        ]\n   [ 0.          0.          0.        ]\n   [ 0.          0.          0.        ]\n   ..., \n   [ 0.          0.          0.        ]\n   [ 0.          0.          0.        ]\n   [ 0.          0.          0.        ]]\n\n  ..., \n  [[ 0.          0.          0.        ]\n   [ 0.          0.          0.        ]\n   [ 0.          0.          0.        ]\n   ..., \n   [ 0.          0.          0.        ]\n   [ 0.          0.          0.        ]\n   [ 0.          0.          0.        ]]\n\n  [[ 0.          0.          0.        ]\n   [ 0.          0.          0.        ]\n   [ 0.          0.          0.        ]\n   ..., \n   [ 0.          0.          0.        ]\n   [ 0.          0.          0.        ]\n   [ 0.          0.          0.        ]]\n\n  [[ 0.          0.          0.        ]\n   [ 0.          0.          0.        ]\n   [ 0.          0.          0.        ]\n   ..., \n   [ 0.          0.          0.        ]\n   [ 0.          0.          0.        ]\n   [ 0.          0.          0.        ]]]\n\n\n [[[ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]\n   ..., \n   [ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]]\n\n  [[ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]\n   ..., \n   [ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]]\n\n  [[ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]\n   ..., \n   [ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]]\n\n  ..., \n  [[ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]\n   ..., \n   [ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]]\n\n  [[ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]\n   ..., \n   [ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]]\n\n  [[ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]\n   ..., \n   [ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]\n   [ 1.          1.          1.        ]]]]"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='tanh', padding='same', input_shape=(128,128,3), name='conv_tanh_1'))\n",
    "model.add(Conv2D(32, (3, 3), activation='tanh', padding='same', name='conv_tanh_2'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), padding='same', name='maxpool_1'))\n",
    "model.add(Dropout(0.25, name='dropout_1'))\n",
    "model.add(Flatten(name='flat_1'))\n",
    "model.add(Dense(512, activation='tanh', name='dense_tanh_1'))\n",
    "model.add(Dropout(0.5, name='dropout_2'))\n",
    "model.add(Dense(101, activation='softmax', name='dense_softmax_1'))\n",
    "model.load_weights('ConvAutoEncoderNET1_weights_l1', by_name=True)\n",
    "#model.load_weights('ConvAutoEncoderNET1_weights_l2', by_name=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "#%%\n",
    "early_stopping=EarlyStopping(monitor='acc', patience=3, verbose=0, mode='auto')\n",
    "t0=time.time()\n",
    "model.fit_generator(train_generator,\n",
    "        steps_per_epoch=train_count/batch,\n",
    "        epochs=epochs,\n",
    "        callbacks=[early_stopping])\n",
    "t1=time.time()\n",
    "loss_and_metrics = model.evaluate_generator(test_generator, steps=test_count/batch)\n",
    "print('Accuracy =',loss_and_metrics[1])\n",
    "print('Time =',(t1-t0))\n",
    "#%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
